{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f42e788",
   "metadata": {},
   "source": [
    "# 3.6 Open-Source Alternative LLMs ‚Äî Transformers, Ollama & HF Inference\n",
    "\n",
    "## Playground Notebook\n",
    "\n",
    "Three ways to run LLMs locally or for free:\n",
    "\n",
    "| Tool | Setup | Cost | Speed | Internet |\n",
    "|------|-------|------|-------|----------|\n",
    "| **Transformers** | `pip install` | Free | Fast | Works offline |\n",
    "| **Ollama** | Download app | Free | Fast | Works offline |\n",
    "| **HF Inference** | Free API token | Free tier | Depends | Requires internet |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32210cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dfe7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def print_block(title, width=60):\n",
    "    \"\"\"Pretty print a section header.\"\"\"\n",
    "    sep = \"=\" * width\n",
    "    print(f\"\\n{sep}\")\n",
    "    print(f\"  {title}\")\n",
    "    print(f\"{sep}\")\n",
    "\n",
    "\n",
    "def benchmark(name, func, *args, **kwargs):\n",
    "    \"\"\"Run a function and measure execution time.\"\"\"\n",
    "    start = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"\\n‚è±Ô∏è  {name}: {elapsed:.2f}s\")\n",
    "    return result, elapsed\n",
    "\n",
    "\n",
    "def show_result(result, prefix=\"\"):\n",
    "    \"\"\"Display result in a readable format.\"\"\"\n",
    "    if isinstance(result, (list, tuple)):\n",
    "        for i, item in enumerate(result, 1):\n",
    "            print(f\"{prefix}[{i}] {item}\")\n",
    "    else:\n",
    "        print(f\"{prefix}{result}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15277f80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. HuggingFace Transformers ‚Äî Local Pipeline\n",
    "\n",
    "**Install:** `pip install transformers torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f0d69c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformers available\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from transformers import pipeline\n",
    "    print(\"‚úÖ Transformers available\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers not installed. Run: pip install transformers torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495951f",
   "metadata": {},
   "source": [
    "### GPU Diagnostic (run this first if CPU-only issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5c7c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  GPU Diagnostic\n",
      "============================================================\n",
      "PyTorch Version: 2.6.0\n",
      "CUDA Available: False\n",
      "CUDA Version: None\n",
      "\n",
      "‚ö†Ô∏è  GPU NOT detected. Installing GPU support...\n",
      "\n",
      "Run this in terminal:\n",
      "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
      "\n",
      "Then restart the kernel (Kernel ‚Üí Restart Kernel)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print_block(\"GPU Diagnostic\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  GPU NOT detected. Installing GPU support...\")\n",
    "    print(\"\\nRun this in terminal:\")\n",
    "    print('pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118')\n",
    "    print(\"\\nThen restart the kernel (Kernel ‚Üí Restart Kernel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c0556",
   "metadata": {},
   "source": [
    "### Experiment 1A: Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48db8e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  1A: Text Generation (DistilGPT-2) ‚Äî Optimized\n",
      "============================================================\n",
      "Device: ‚ùå CPU only\n",
      "\n",
      "Loading model (first time only)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8b67f376314fb6ba64c5ff277f9691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model cached for future use\n",
      "\n",
      "\n",
      "‚è±Ô∏è  Generation: 1.47s\n",
      "\n",
      "üìù Input: The future of artificial intelligence\n",
      "\n",
      "üìÑ Output:\n",
      "------------------------------------------------------------\n",
      "The future of artificial intelligence is in its infancy. The future of artificial intelligence is in its infancy. The future of artificial intelligence is in its infancy. The future of artificial intelligence\n",
      "------------------------------------------------------------\n",
      "\n",
      "üí° Tips to make it faster:\n",
      "   1. GPU: Install NVIDIA drivers + 'pip install torch --upgrade'\n",
      "   2. First run caches model ‚Äî subsequent runs are much faster\n",
      "   3. Reduce max_new_tokens for quicker generation\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "print_block(\"1A: Text Generation (DistilGPT-2) ‚Äî Optimized\")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "gpu_status = \"‚úÖ GPU\" if device == 0 else \"‚ùå CPU only\"\n",
    "print(f\"Device: {gpu_status}\\n\")\n",
    "\n",
    "# Load the model with device mapping (only load once)\n",
    "if not hasattr(globals(), 'generator'):\n",
    "    print(\"Loading model (first time only)...\")\n",
    "    generator = pipeline(\n",
    "        'text-generation',\n",
    "        model='distilgpt2',\n",
    "        device=device\n",
    "    )\n",
    "    print(\"‚úÖ Model cached for future use\\n\")\n",
    "\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "\n",
    "# Optimized generation parameters\n",
    "result, elapsed = benchmark(\n",
    "    \"Generation\",\n",
    "    generator,\n",
    "    prompt,\n",
    "    max_new_tokens=30,         # Fixed: use max_new_tokens instead of max_length\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,            # Lower = more consistent\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Input: {prompt}\")\n",
    "print(f\"\\nüìÑ Output:\")\n",
    "print(\"-\" * 60)\n",
    "print(result[0]['generated_text'].strip())\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(f\"\\nüí° Tips to make it faster:\")\n",
    "print(\"   1. GPU: Install NVIDIA drivers + 'pip install torch --upgrade'\")\n",
    "print(\"   2. First run caches model ‚Äî subsequent runs are much faster\")\n",
    "print(\"   3. Reduce max_new_tokens for quicker generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6a07258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "CUDA available: False\n",
      "CUDA device: None\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# To enable GPU, install:\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7081c92a",
   "metadata": {},
   "source": [
    "### Experiment 1B: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dea01be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  1B: Sentiment Analysis (DistilBERT)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6e7acbb1034ecf9cce35e8a3a0558d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è  Analyze: 'This product is amazing!'...: 0.07s\n",
      "   ‚Üí POSITIVE (99.99%)\n",
      "\n",
      "‚è±Ô∏è  Analyze: 'Terrible experience, very disappointed.'...: 0.01s\n",
      "   ‚Üí NEGATIVE (99.98%)\n",
      "\n",
      "‚è±Ô∏è  Analyze: 'It's okay, nothing special.'...: 0.02s\n",
      "   ‚Üí NEGATIVE (81.90%)\n"
     ]
    }
   ],
   "source": [
    "print_block(\"1B: Sentiment Analysis (DistilBERT)\")\n",
    "\n",
    "sentiment_analyzer = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english'\n",
    ")\n",
    "\n",
    "texts = [\n",
    "    \"This product is amazing!\",\n",
    "    \"Terrible experience, very disappointed.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result, elapsed = benchmark(\n",
    "        f\"Analyze: '{text}'...\",\n",
    "        sentiment_analyzer,\n",
    "        text\n",
    "    )\n",
    "    \n",
    "    sentiment = result[0]['label']\n",
    "    score = result[0]['score']\n",
    "    print(f\"   ‚Üí {sentiment} ({score:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380333f",
   "metadata": {},
   "source": [
    "### Experiment 1C: Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b63e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  1C: Question Answering (DistilBERT)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d60547cf1c46edb295942f8e49b7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Context: Python is a programming language created by Guido van Rossum in 1989.\n",
      "‚ùì Question: Who created Python?\n",
      "\n",
      "‚è±Ô∏è  Q&A: 0.09s\n",
      "\n",
      "üí° Answer: Guido van Rossum (confidence: 99.55%)\n"
     ]
    }
   ],
   "source": [
    "print_block(\"1C: Question Answering (DistilBERT)\")\n",
    "\n",
    "qa = pipeline(\n",
    "    'question-answering',\n",
    "    model='distilbert-base-cased-distilled-squad'\n",
    ")\n",
    "\n",
    "context = \"Python is a programming language created by Guido van Rossum in 1989.\"\n",
    "question = \"Who created Python?\"\n",
    "\n",
    "print(f\"üìö Context: {context}\")\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "\n",
    "result, elapsed = benchmark(\n",
    "    \"Q&A\",\n",
    "    qa,\n",
    "    question=question,\n",
    "    context=context\n",
    ")\n",
    "\n",
    "print(f\"\\nüí° Answer: {result['answer']} (confidence: {result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8eef80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Ollama ‚Äî Run LLMs Locally\n",
    "\n",
    "**Install:** Download from [ollama.ai](https://ollama.ai)\n",
    "\n",
    "**Run:** `ollama serve` then in another terminal `ollama run llama2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b7a7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama package available\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is available\n",
    "try:\n",
    "    import ollama\n",
    "    print(\"‚úÖ Ollama package available\")\n",
    "    HAS_OLLAMA = True\n",
    "except ImportError:\n",
    "    print(\"‚ùå Ollama package not installed. Install with: pip install ollama\")\n",
    "    HAS_OLLAMA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12079e",
   "metadata": {},
   "source": [
    "### Experiment 2A: Ollama Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59dc3419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  2A: Ollama Text Generation\n",
      "============================================================\n",
      "\n",
      "‚è±Ô∏è  Ollama generate: 2.40s\n",
      "\n",
      "üìù Prompt: Write a haiku about coding:\n",
      "\n",
      "üìÑ Response:\n",
      "------------------------------------------------------------\n",
      "Byte by byte,\n",
      "Lines of code come alive,\n",
      "Programs run free.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def ollama_generate(prompt, model='qwen2.5:1.5b'):\n",
    "    \"\"\"Generate text using Ollama.\"\"\"\n",
    "    response = ollama.generate(model=model, prompt=prompt)\n",
    "    return response['response']\n",
    "\n",
    "\n",
    "print_block(\"2A: Ollama Text Generation\")\n",
    "\n",
    "if HAS_OLLAMA:\n",
    "    prompt = \"Write a haiku about coding:\"\n",
    "    result, elapsed = benchmark(\n",
    "        \"Ollama generate\",\n",
    "        ollama_generate,\n",
    "        prompt\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(f\"\\nüìÑ Response:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(result.strip())\n",
    "    print(\"-\" * 60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ollama package not available. Skipping this experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501397cd",
   "metadata": {},
   "source": [
    "### Experiment 2B: List Available Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dc344a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  2B: List Available Models\n",
      "============================================================\n",
      "\n",
      "ü§ñ Installed Models:\n",
      "   ‚Ä¢ gpt-oss:20b (12.8 GB)\n",
      "   ‚Ä¢ qwen2.5:1.5b (0.9 GB)\n",
      "   ‚Ä¢ gemma3n:e2b (5.2 GB)\n",
      "   ‚Ä¢ deepseek-r1:8b (4.6 GB)\n",
      "   ‚Ä¢ nomic-embed-text:latest (0.3 GB)\n",
      "   ‚Ä¢ jina/jina-embeddings-v2-base-en:latest (0.3 GB)\n",
      "   ‚Ä¢ all-minilm:latest (0.0 GB)\n"
     ]
    }
   ],
   "source": [
    "def ollama_list_models():\n",
    "    \"\"\"Get list of installed Ollama models.\"\"\"\n",
    "    try:\n",
    "        models_list = ollama.list()\n",
    "        return models_list.get('models', [])\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "print_block(\"2B: List Available Models\")\n",
    "\n",
    "if HAS_OLLAMA:\n",
    "    models = ollama_list_models()\n",
    "    if models:\n",
    "        print(f\"\\nü§ñ Installed Models:\")\n",
    "        for model in models:\n",
    "            name = model.get('model', 'Unknown')\n",
    "            size = model.get('size', 0)\n",
    "            size_gb = size / (1024**3)\n",
    "            print(f\"   ‚Ä¢ {name} ({size_gb:.1f} GB)\")\n",
    "    else:\n",
    "        print(\"   No models installed. Run: ollama pull qwen2.5:1.5b\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ollama package not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75376c20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. HuggingFace Inference API ‚Äî Free Cloud Endpoint\n",
    "\n",
    "**Setup:** Get free token from [huggingface.co](https://huggingface.co) ‚Üí Settings ‚Üí Access Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c57885e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  3. HuggingFace Inference API\n",
      "============================================================\n",
      "\n",
      "üîë Token status: ‚úÖ Set from .env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get HF token from .env or environment variable\n",
    "HF_TOKEN = os.getenv('HUGGING_FACE_HUB_TOKEN')\n",
    "\n",
    "# Initialize the official HF Inference Client\n",
    "hf_client = InferenceClient(token=HF_TOKEN) if HF_TOKEN else None\n",
    "\n",
    "def hf_inference(text, model_name='openai-community/gpt2', task='text_generation'):\n",
    "    \"\"\"Call HuggingFace Inference API using the official client.\"\"\"\n",
    "    if not hf_client:\n",
    "        return {\"error\": \"HF_TOKEN not set\"}\n",
    "    try:\n",
    "        if task == 'text_generation':\n",
    "            result = hf_client.text_generation(text, model=model_name, max_new_tokens=50)\n",
    "            return [{\"generated_text\": text + result}]\n",
    "        elif task == 'text_classification':\n",
    "            result = hf_client.text_classification(text, model=model_name)\n",
    "            return result\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown task: {task}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "print_block(\"3. HuggingFace Inference API\")\n",
    "token_status = \"‚úÖ Set from .env\" if HF_TOKEN else \"‚ùå Not set\"\n",
    "print(f\"\\nüîë Token status: {token_status}\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"   Create .env file with: HUGGING_FACE_HUB_TOKEN=your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1864f",
   "metadata": {},
   "source": [
    "### Experiment 3A: Sentiment Analysis via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96fc9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  3B: HF API Sentiment Analysis\n",
      "============================================================\n",
      "\n",
      "‚è±Ô∏è  Sentiment: 'I love this!'...: 0.53s\n",
      "   ‚Üí ‚ùå Model 'distilbert/distilbert-base-uncased-finetuned-sst-2-english' doesn't support task 'text-generation'. Supported tasks: 'text-classification', got: 'text-generation'\n",
      "\n",
      "‚è±Ô∏è  Sentiment: 'This is terrible.'...: 0.27s\n",
      "   ‚Üí ‚ùå Model 'distilbert/distilbert-base-uncased-finetuned-sst-2-english' doesn't support task 'text-generation'. Supported tasks: 'text-classification', got: 'text-generation'\n"
     ]
    }
   ],
   "source": [
    "print_block(\"3B: HF API Sentiment Analysis\")\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    print(\"‚ö†Ô∏è  Please set HUGGING_FACE_HUB_TOKEN in .env file.\")\n",
    "else:\n",
    "    texts = [\n",
    "        \"I love this!\",\n",
    "        \"This is terrible.\"\n",
    "    ]\n",
    "    \n",
    "    for text in texts:\n",
    "        result, elapsed = benchmark(\n",
    "            f\"Sentiment: '{text}'...\",\n",
    "            hf_inference,\n",
    "            text,\n",
    "            'distilbert/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "        )\n",
    "        \n",
    "        if isinstance(result, dict) and 'error' in result:\n",
    "            print(f\"   ‚Üí ‚ùå {result['error']}\")\n",
    "        elif isinstance(result, list) and len(result) > 0:\n",
    "            # HF returns [[{label, score}, ...]] for classification\n",
    "            top = result[0] if isinstance(result[0], dict) else result[0][0]\n",
    "            print(f\"   ‚Üí {top.get('label', '?')} ({top.get('score', 0):.2%})\")\n",
    "        else:\n",
    "            print(f\"   ‚Üí {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822a1a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison\n",
    "\n",
    "| Aspect | Transformers | Ollama | HF API |\n",
    "|--------|--------------|--------|--------|\n",
    "| **Setup** | pip install | Download app | Web login |\n",
    "| **Cost** | Free | Free | Free tier |\n",
    "| **Speed** | Fast | Fast | Network latency |\n",
    "| **Offline** | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No |\n",
    "| **Customizable** | ‚úÖ Full fine-tuning | ‚ö†Ô∏è Limited | ‚ùå No |\n",
    "| **Memory** | Depends on model | Depends on model | None (cloud) |\n",
    "| **Best for** | Production, learning | Local development | Quick testing |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | Remember |\n",
    "|---------|----------|\n",
    "| **Transformers** | Easy pipelines for any NLP task, fully local |\n",
    "| **Ollama** | One-command LLM setup, great for development |\n",
    "| **HF Inference** | Quick cloud testing without managing infrastructure |\n",
    "| **No API keys** | Transformers & Ollama are totally free |\n",
    "| **Batch processing** | Use lists for efficiency |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
