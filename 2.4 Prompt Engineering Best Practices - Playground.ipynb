{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 2.4 Prompt Engineering Best Practices\n",
    "\n",
    "## Playground Notebook\n",
    "\n",
    "This notebook covers **production-grade best practices** for crafting, testing, and managing prompts at scale.\n",
    "\n",
    "| Topic | Core Idea |\n",
    "|-------|-----------|\n",
    "| **Iterative Refinement** | Systematically improve prompts through scored feedback loops |\n",
    "| **A/B Testing** | Measure and compare prompt variants with objective metrics |\n",
    "| **Hallucination Handling** | Detect and reduce fabricated facts through grounding techniques |\n",
    "| **Bias Detection & Mitigation** | Identify and neutralise unfair assumptions in prompts and outputs |\n",
    "| **Documentation & Version Control** | Track, version, and audit prompts like production code |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using model: gpt-oss:20b-cloud\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "import hashlib\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "from collections import defaultdict\n",
    "from statistics import mean, stdev\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# ============================================================\n",
    "#  CONFIGURATION\n",
    "# ============================================================\n",
    "MODEL = \"gpt-oss:20b-cloud\"  # qwen2.5:1.5b -> alternate model\n",
    "\n",
    "llm = ChatOllama(model=MODEL)\n",
    "\n",
    "# ============================================================\n",
    "#  HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def build_messages(message_dicts):\n",
    "    \"\"\"Convert role/content dicts into LangChain message objects.\"\"\"\n",
    "    type_map = {\"system\": SystemMessage, \"user\": HumanMessage, \"assistant\": AIMessage}\n",
    "    return [type_map[m[\"role\"]](content=m[\"content\"]) for m in message_dicts]\n",
    "\n",
    "\n",
    "def chat(messages, show=True, **kwargs):\n",
    "    \"\"\"Send messages to the model and return the response text.\"\"\"\n",
    "    _llm = ChatOllama(model=MODEL, **kwargs) if kwargs else llm\n",
    "    lc_messages = build_messages(messages)\n",
    "    start = time.time()\n",
    "    response = _llm.invoke(lc_messages)\n",
    "    elapsed = time.time() - start\n",
    "    content = response.content\n",
    "    if show:\n",
    "        display(Markdown(content))\n",
    "        print(f\"\\n‚è±Ô∏è {elapsed:.2f}s | {len(content)} chars\")\n",
    "    return content\n",
    "\n",
    "\n",
    "def show_messages(messages):\n",
    "    \"\"\"Pretty-print the message list.\"\"\"\n",
    "    colors = {\"system\": \"#e74c3c\", \"user\": \"#3498db\", \"assistant\": \"#2ecc71\"}\n",
    "    html = \"\"\n",
    "    for msg in messages:\n",
    "        role = msg[\"role\"]\n",
    "        color = colors.get(role, \"#888\")\n",
    "        preview = msg[\"content\"][:400] + (\"...\" if len(msg[\"content\"]) > 400 else \"\")\n",
    "        html += (\n",
    "            f'<div style=\"margin:6px 0;padding:8px 12px;border-left:4px solid {color};'\n",
    "            f'background:#1e1e1e;border-radius:4px;\">'\n",
    "            f'<strong style=\"color:{color};text-transform:uppercase;\">{role}</strong>'\n",
    "            f'<br><span style=\"color:#ccc;white-space:pre-wrap;\">{preview}</span></div>'\n",
    "        )\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def section_header(title, subtitle=\"\"):\n",
    "    html = f\"\"\"\n",
    "    <div style=\"background:linear-gradient(135deg,#1a1a2e,#16213e);padding:16px 20px;\n",
    "                border-radius:8px;border-left:5px solid #e94560;margin:20px 0;\">\n",
    "      <h2 style=\"color:#e94560;margin:0;\">{title}</h2>\n",
    "      <p style=\"color:#aaa;margin:6px 0 0;\">{subtitle}</p>\n",
    "    </div>\"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def score_card(label, scores: dict, highlight_best=True):\n",
    "    \"\"\"Render a colour-coded score card.\"\"\"\n",
    "    best_key = max(scores, key=scores.get) if highlight_best else None\n",
    "    html = f'<div style=\"margin:10px 0;\"><strong style=\"color:#e94560;\">{label}</strong><br>'\n",
    "    for k, v in scores.items():\n",
    "        bar_w = int(v * 10)  # assume 0‚Äì10 scale\n",
    "        color = \"#2ecc71\" if k == best_key else \"#3498db\"\n",
    "        html += (\n",
    "            f'<div style=\"margin:4px 0;display:flex;align-items:center;gap:8px;\">'\n",
    "            f'<span style=\"color:#ccc;min-width:160px;\">{k}</span>'\n",
    "            f'<div style=\"background:{color};width:{bar_w * 12}px;height:14px;border-radius:3px;\"></div>'\n",
    "            f'<span style=\"color:#fff;\">{v:.1f}/10</span></div>'\n",
    "        )\n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Iterative Prompt Refinement Methodology\n",
    "\n",
    "**Iterative refinement** treats prompt engineering as an empirical process ‚Äî start with a baseline, measure what's wrong, then make targeted improvements.\n",
    "\n",
    "```\n",
    "Baseline Prompt\n",
    "     ‚Üì\n",
    "Run & Observe Output\n",
    "     ‚Üì\n",
    "Identify Failure Modes  ‚Üê score against criteria\n",
    "     ‚Üì\n",
    "Targeted Improvement\n",
    "     ‚Üì\n",
    "Run & Compare  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚Üì                            ‚îÇ\n",
    "Satisfactory? ‚îÄ‚îÄ No ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "     ‚Üì Yes\n",
    "Final Prompt\n",
    "```\n",
    "\n",
    "### Experiment 1A: Baseline ‚Üí Critique ‚Üí Refined Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  BASELINE PROMPT\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"margin:6px 0;padding:8px 12px;border-left:4px solid #e74c3c;background:#1e1e1e;border-radius:4px;\"><strong style=\"color:#e74c3c;text-transform:uppercase;\">system</strong><br><span style=\"color:#ccc;white-space:pre-wrap;\">You are a helpful assistant. Summarise the following text.</span></div><div style=\"margin:6px 0;padding:8px 12px;border-left:4px solid #3498db;background:#1e1e1e;border-radius:4px;\"><strong style=\"color:#3498db;text-transform:uppercase;\">user</strong><br><span style=\"color:#ccc;white-space:pre-wrap;\">\n",
       "Transformer models use a self-attention mechanism that computes pairwise similarity scores\n",
       "between all tokens in a sequence. The attention weights are derived from query, key, and\n",
       "value projections of the input embeddings. Multi-head attention allows the model to attend\n",
       "to different representation subspaces simultaneously. Positional encodings are added to\n",
       "preserve sequence order information sinc...</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ResponseError",
     "evalue": "model 'gpt-oss:20b-cloud' not found (status code: 404)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResponseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m baseline_msgs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: BASELINE_SYSTEM},\n\u001b[1;32m     20\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SAMPLE_ARTICLE}\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     22\u001b[0m show_messages(baseline_msgs)\n\u001b[0;32m---> 23\u001b[0m baseline_output \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_msgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(messages, show, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m lc_messages \u001b[38;5;241m=\u001b[39m build_messages(messages)\n\u001b[1;32m     34\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43m_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlc_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     37\u001b[0m content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:402\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    397\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    400\u001b[0m         cast(\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 402\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    412\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m    413\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1119\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1120\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:931\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    930\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 931\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m         )\n\u001b[1;32m    938\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    939\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1233\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_ollama/chat_models.py:1030\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m   1024\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1025\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1029\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m-> 1030\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m   1034\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m   1035\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m   1036\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1044\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_ollama/chat_models.py:965\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    958\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    963\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    964\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_ollama/chat_models.py:1054\u001b[0m, in \u001b[0;36mChatOllama._iterate_over_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iterate_over_stream\u001b[39m(\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1049\u001b[0m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[1;32m   1050\u001b[0m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1052\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ChatGenerationChunk]:\n\u001b[1;32m   1053\u001b[0m     reasoning \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreasoning)\n\u001b[0;32m-> 1054\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/langchain_ollama/chat_models.py:952\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client:\n\u001b[0;32m--> 952\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.13/site-packages/ollama/_client.py:179\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    178\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 179\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m    182\u001b[0m   part \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "\u001b[0;31mResponseError\u001b[0m: model 'gpt-oss:20b-cloud' not found (status code: 404)"
     ]
    }
   ],
   "source": [
    "# Task: Summarise a technical article for a non-technical audience\n",
    "SAMPLE_ARTICLE = \"\"\"\n",
    "Transformer models use a self-attention mechanism that computes pairwise similarity scores\n",
    "between all tokens in a sequence. The attention weights are derived from query, key, and\n",
    "value projections of the input embeddings. Multi-head attention allows the model to attend\n",
    "to different representation subspaces simultaneously. Positional encodings are added to\n",
    "preserve sequence order information since the attention operation itself is permutation-\n",
    "invariant. Feed-forward sublayers apply non-linear transformations independently to each\n",
    "position. Layer normalisation and residual connections stabilise training.\n",
    "\"\"\"\n",
    "\n",
    "# ---- BASELINE PROMPT (vague, no format, no audience guidance) ----\n",
    "BASELINE_SYSTEM = \"You are a helpful assistant. Summarise the following text.\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  BASELINE PROMPT\")\n",
    "print(\"=\" * 60)\n",
    "baseline_msgs = [\n",
    "    {\"role\": \"system\", \"content\": BASELINE_SYSTEM},\n",
    "    {\"role\": \"user\", \"content\": SAMPLE_ARTICLE}\n",
    "]\n",
    "show_messages(baseline_msgs)\n",
    "baseline_output = chat(baseline_msgs, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CRITIQUE: Score the baseline output against criteria ----\n",
    "CRITERIA = [\n",
    "    \"clarity_for_non_technical_reader (0‚Äì10)\",\n",
    "    \"accuracy_of_key_concepts (0‚Äì10)\",\n",
    "    \"appropriate_length (0‚Äì10)\",\n",
    "    \"avoidance_of_jargon (0‚Äì10)\",\n",
    "]\n",
    "\n",
    "critique_msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a prompt quality evaluator. Score the response on each criterion (0‚Äì10) \"\n",
    "            \"and explain what to fix. Return JSON only:\\n\"\n",
    "            '{\"scores\": {\"criterion\": score, ...}, \"issues\": [\"issue1\", \"issue2\", ...], '\n",
    "            '\"improvement_suggestions\": [\"suggestion1\", ...]}'\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Task: Summarise a technical AI article for a complete non-technical beginner.\\n\"\n",
    "            f\"Criteria: {', '.join(CRITERIA)}\\n\\n\"\n",
    "            f\"Response to evaluate:\\n{baseline_output}\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"CRITIQUE ‚Äî Scoring baseline output\")\n",
    "print(\"=\" * 60)\n",
    "critique_raw = chat(critique_msgs, show=False, temperature=0.1)\n",
    "\n",
    "# Parse critique\n",
    "json_match = re.search(r'\\{[\\s\\S]*\\}', critique_raw)\n",
    "critique = json.loads(json_match.group()) if json_match else {}\n",
    "scores = critique.get(\"scores\", {})\n",
    "issues = critique.get(\"issues\", [])\n",
    "suggestions = critique.get(\"improvement_suggestions\", [])\n",
    "\n",
    "score_card(\"Baseline Scores\", {k: float(v) for k, v in scores.items()})\n",
    "print(\"\\nüî¥ Issues identified:\")\n",
    "for i in issues:\n",
    "    print(f\"  ‚Ä¢ {i}\")\n",
    "print(\"\\nüí° Suggestions:\")\n",
    "for s in suggestions:\n",
    "    print(f\"  ‚Üí {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- REFINED PROMPT ‚Äî incorporate critique findings ----\n",
    "# Use the model to auto-generate the improved prompt based on the critique\n",
    "refine_msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a prompt engineer. Rewrite the given system prompt to fix the identified issues. \"\n",
    "            \"Output ONLY the improved system prompt text.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Original system prompt:\\n{BASELINE_SYSTEM}\\n\\n\"\n",
    "            f\"Issues:\\n\" + \"\\n\".join(f\"- {i}\" for i in issues) + \"\\n\\n\"\n",
    "            f\"Suggestions:\\n\" + \"\\n\".join(f\"- {s}\" for s in suggestions) + \"\\n\\n\"\n",
    "            \"Write the improved system prompt:\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "REFINED_SYSTEM = chat(refine_msgs, show=False, temperature=0.3)\n",
    "print(\"REFINED SYSTEM PROMPT:\")\n",
    "print(\"=\" * 60)\n",
    "display(Markdown(f\"```\\n{REFINED_SYSTEM}\\n```\"))\n",
    "\n",
    "# Run refined prompt on same input\n",
    "print(\"\\nREFINED OUTPUT:\")\n",
    "print(\"=\" * 60)\n",
    "refined_msgs = [\n",
    "    {\"role\": \"system\", \"content\": REFINED_SYSTEM},\n",
    "    {\"role\": \"user\", \"content\": SAMPLE_ARTICLE}\n",
    "]\n",
    "show_messages(refined_msgs)\n",
    "refined_output = chat(refined_msgs, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- SCORE REFINED OUTPUT and compare ----\n",
    "refined_critique_msgs = [\n",
    "    critique_msgs[0],\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"Task: Summarise a technical AI article for a complete non-technical beginner.\\n\"\n",
    "            f\"Criteria: {', '.join(CRITERIA)}\\n\\n\"\n",
    "            f\"Response to evaluate:\\n{refined_output}\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "refined_critique_raw = chat(refined_critique_msgs, show=False, temperature=0.1)\n",
    "json_match2 = re.search(r'\\{[\\s\\S]*\\}', refined_critique_raw)\n",
    "refined_scores = json.loads(json_match2.group()).get(\"scores\", {}) if json_match2 else {}\n",
    "\n",
    "print(\"BEFORE vs AFTER ‚Äî Score Comparison\")\n",
    "print(\"=\" * 60)\n",
    "score_card(\"Baseline\", {k: float(v) for k, v in scores.items()})\n",
    "score_card(\"Refined\", {k: float(v) for k, v in refined_scores.items()})\n",
    "\n",
    "if scores and refined_scores:\n",
    "    b_avg = mean(float(v) for v in scores.values())\n",
    "    r_avg = mean(float(v) for v in refined_scores.values())\n",
    "    delta = r_avg - b_avg\n",
    "    print(f\"\\nüìà Average score: Baseline {b_avg:.1f} ‚Üí Refined {r_avg:.1f} (Œî {delta:+.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Experiment 1B: Multi-Round Automated Refinement Loop\n",
    "\n",
    "Instead of a single refinement, run **N rounds** automatically until quality crosses a threshold or stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_refine(task_description: str, initial_prompt: str, test_input: str,\n",
    "                criteria: list, target_score: float = 8.0, max_rounds: int = 3):\n",
    "    \"\"\"\n",
    "    Automatically refine a prompt until the average score hits target_score\n",
    "    or max_rounds is reached.\n",
    "    \"\"\"\n",
    "    history = []  # track (round, prompt, avg_score)\n",
    "    current_prompt = initial_prompt\n",
    "\n",
    "    for rnd in range(1, max_rounds + 1):\n",
    "        print(f\"\\n{'‚îÄ' * 60}\")\n",
    "        print(f\"  ROUND {rnd}\")\n",
    "        print(f\"{'‚îÄ' * 60}\")\n",
    "\n",
    "        # 1. Run current prompt\n",
    "        output = chat(\n",
    "            [{\"role\": \"system\", \"content\": current_prompt}, {\"role\": \"user\", \"content\": test_input}],\n",
    "            show=False, temperature=0.3\n",
    "        )\n",
    "        display(Markdown(f\"**Round {rnd} output (truncated):** {output[:300]}...\"))\n",
    "\n",
    "        # 2. Score it\n",
    "        score_raw = chat([\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    f\"Score the response on each criterion (0‚Äì10). Task: {task_description}. \"\n",
    "                    'Return JSON: {\"scores\": {criterion: score}, \"issues\": [...], \"suggestions\": [...]}'\n",
    "                )\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"Criteria: {criteria}\\n\\nResponse:\\n{output}\"}\n",
    "        ], show=False, temperature=0.1)\n",
    "\n",
    "        m = re.search(r'\\{[\\s\\S]*\\}', score_raw)\n",
    "        data = json.loads(m.group()) if m else {}\n",
    "        rnd_scores = {k: float(v) for k, v in data.get(\"scores\", {}).items()}\n",
    "        issues = data.get(\"issues\", [])\n",
    "        suggestions = data.get(\"suggestions\", [])\n",
    "\n",
    "        avg = mean(rnd_scores.values()) if rnd_scores else 0.0\n",
    "        score_card(f\"Round {rnd} scores (avg {avg:.1f})\", rnd_scores)\n",
    "        history.append((rnd, current_prompt[:80], avg))\n",
    "\n",
    "        if avg >= target_score:\n",
    "            print(f\"\\n‚úÖ Target score {target_score} reached at round {rnd}!\")\n",
    "            break\n",
    "\n",
    "        # 3. Refine prompt\n",
    "        current_prompt = chat([\n",
    "            {\"role\": \"system\", \"content\": \"You are a prompt engineer. Rewrite the system prompt to fix the issues. Output ONLY the improved prompt.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Prompt:\\n{current_prompt}\\n\\nIssues:\\n{issues}\\n\\nSuggestions:\\n{suggestions}\"}\n",
    "        ], show=False, temperature=0.3)\n",
    "\n",
    "    print(\"\\nüìä Refinement History:\")\n",
    "    for r, p, s in history:\n",
    "        print(f\"  Round {r}: avg={s:.1f} | prompt_start='{p}...'\")\n",
    "    return current_prompt\n",
    "\n",
    "\n",
    "WEAK_PROMPT = \"Write a product description.\"\n",
    "PRODUCT_INFO = \"\"\"Product: EcoBreeze Air Purifier. Features: HEPA + activated carbon filter,\n",
    "quiet (22dB), covers 500 sq ft, smart app control, auto mode. Price: $189.\"\"\"\n",
    "\n",
    "print(\"AUTO-REFINE LOOP\")\n",
    "print(\"=\" * 60)\n",
    "final_prompt = auto_refine(\n",
    "    task_description=\"Write a compelling e-commerce product description that drives conversions\",\n",
    "    initial_prompt=WEAK_PROMPT,\n",
    "    test_input=PRODUCT_INFO,\n",
    "    criteria=[\"persuasiveness\", \"clarity\", \"feature_coverage\", \"call_to_action\"],\n",
    "    target_score=7.5,\n",
    "    max_rounds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. A/B Testing Prompts for Optimization\n",
    "\n",
    "**A/B testing** applies the scientific method to prompts: define variants, define metrics, run experiments, and let data decide which prompt wins.\n",
    "\n",
    "```\n",
    "Variant A (control)  ‚îê\n",
    "                     ‚îú‚îÄ‚îÄ same inputs ‚Üí outputs ‚Üí score each ‚Üí compare\n",
    "Variant B (treatment)‚îò\n",
    "```\n",
    "\n",
    "### Experiment 2A: Head-to-Head Prompt Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two prompt variants for a customer email response task\n",
    "VARIANT_A = {\n",
    "    \"name\": \"Variant A ‚Äî Generic\",\n",
    "    \"system\": \"You are a customer support agent. Reply to the customer email professionally.\"\n",
    "}\n",
    "\n",
    "VARIANT_B = {\n",
    "    \"name\": \"Variant B ‚Äî Structured Role\",\n",
    "    \"system\": (\n",
    "        \"You are a senior customer support specialist at a software company. \"\n",
    "        \"When replying to customer emails:\\n\"\n",
    "        \"1. Acknowledge the customer's specific issue in the first sentence\\n\"\n",
    "        \"2. Provide a clear, actionable resolution or next step\\n\"\n",
    "        \"3. Set expectations (timeframe, what they'll receive)\\n\"\n",
    "        \"4. Close with a personalised, warm sign-off\\n\"\n",
    "        \"Keep responses under 150 words. Use plain English, no jargon.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Test inputs ‚Äî realistic customer emails\n",
    "TEST_EMAILS = [\n",
    "    \"I've been trying to log in for 3 days but keep getting 'invalid credentials'. I've reset my password twice. This is really frustrating.\",\n",
    "    \"My invoice shows I was charged twice for last month's subscription. Can you please sort this out?\",\n",
    "    \"I can't figure out how to export my data to CSV. The export button doesn't seem to do anything.\"\n",
    "]\n",
    "\n",
    "ab_results = {\"Variant A ‚Äî Generic\": [], \"Variant B ‚Äî Structured Role\": []}\n",
    "\n",
    "JUDGE_SYSTEM = \"\"\"\n",
    "You are a customer service quality evaluator. Score the response (0‚Äì10) on:\n",
    "- empathy: acknowledges the customer's frustration\n",
    "- clarity: is the resolution easy to understand?\n",
    "- actionability: does it tell the customer exactly what to do or expect?\n",
    "- brevity: is it appropriately concise?\n",
    "Return JSON: {\"empathy\": n, \"clarity\": n, \"actionability\": n, \"brevity\": n}\n",
    "\"\"\"\n",
    "\n",
    "for email_idx, email in enumerate(TEST_EMAILS):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"TEST INPUT {email_idx + 1}: {email[:80]}...\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    for variant in [VARIANT_A, VARIANT_B]:\n",
    "        output = chat(\n",
    "            [{\"role\": \"system\", \"content\": variant[\"system\"]},\n",
    "             {\"role\": \"user\", \"content\": email}],\n",
    "            show=False, temperature=0.4\n",
    "        )\n",
    "\n",
    "        score_raw = chat([\n",
    "            {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": f\"Customer email:\\n{email}\\n\\nResponse:\\n{output}\"}\n",
    "        ], show=False, temperature=0.1)\n",
    "\n",
    "        m = re.search(r'\\{[\\s\\S]*?\\}', score_raw)\n",
    "        if m:\n",
    "            scores_dict = {k: float(v) for k, v in json.loads(m.group()).items()}\n",
    "            avg = mean(scores_dict.values())\n",
    "            ab_results[variant[\"name\"]].append(avg)\n",
    "            print(f\"  {variant['name']}: avg={avg:.1f} | {scores_dict}\")\n",
    "            print(f\"  Output: {output[:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"A/B TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for variant_name, run_scores in ab_results.items():\n",
    "    if run_scores:\n",
    "        overall = mean(run_scores)\n",
    "        print(f\"  {variant_name}: overall avg = {overall:.2f}\")\n",
    "\n",
    "winner = max(ab_results, key=lambda k: mean(ab_results[k]) if ab_results[k] else 0)\n",
    "print(f\"\\nüèÜ Winner: {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Experiment 2B: Multi-Variant Testing with Metrics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-variant A/B test: code explanation task\n",
    "CODE_SNIPPET = \"\"\"\n",
    "def quicksort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quicksort(left) + middle + quicksort(right)\n",
    "\"\"\"\n",
    "\n",
    "VARIANTS = [\n",
    "    {\n",
    "        \"name\": \"Plain\",\n",
    "        \"system\": \"Explain this Python code.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Beginner-Focused\",\n",
    "        \"system\": (\n",
    "            \"Explain this Python code to someone who just started programming. \"\n",
    "            \"Use a simple analogy. Avoid technical jargon. Under 100 words.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Structured Expert\",\n",
    "        \"system\": (\n",
    "            \"You are a Python instructor. Explain the code under these headers: \"\n",
    "            \"'What it does', 'How it works (step by step)', 'Time complexity', 'When to use it'. \"\n",
    "            \"Be concise and precise.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Socratic\",\n",
    "        \"system\": (\n",
    "            \"Explain this code by asking and answering questions: \"\n",
    "            \"'What problem does this solve?', 'How does the pivot work?', \"\n",
    "            \"'What happens with duplicates?', 'What is the complexity?'\"\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "CODE_JUDGE = \"\"\"\n",
    "Score the code explanation (0‚Äì10) on:\n",
    "- correctness: is the explanation technically accurate?\n",
    "- clarity: is it easy to understand?\n",
    "- completeness: does it cover the key concepts?\n",
    "- engagement: is it interesting to read?\n",
    "Return JSON: {\"correctness\": n, \"clarity\": n, \"completeness\": n, \"engagement\": n}\n",
    "\"\"\"\n",
    "\n",
    "variant_results = {}\n",
    "\n",
    "print(\"MULTI-VARIANT A/B TEST ‚Äî Code Explanation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for v in VARIANTS:\n",
    "    output = chat(\n",
    "        [{\"role\": \"system\", \"content\": v[\"system\"]},\n",
    "         {\"role\": \"user\", \"content\": CODE_SNIPPET}],\n",
    "        show=False, temperature=0.3\n",
    "    )\n",
    "    score_raw = chat([\n",
    "        {\"role\": \"system\", \"content\": CODE_JUDGE},\n",
    "        {\"role\": \"user\", \"content\": f\"Explanation:\\n{output}\"}\n",
    "    ], show=False, temperature=0.1)\n",
    "\n",
    "    m = re.search(r'\\{[\\s\\S]*?\\}', score_raw)\n",
    "    if m:\n",
    "        s = {k: float(val) for k, val in json.loads(m.group()).items()}\n",
    "        variant_results[v[\"name\"]] = {\"scores\": s, \"avg\": mean(s.values()), \"output\": output}\n",
    "        print(f\"\\n  Variant: {v['name']} (avg={mean(s.values()):.1f})\")\n",
    "        print(f\"  Scores: {s}\")\n",
    "        print(f\"  Output preview: {output[:140]}...\")\n",
    "\n",
    "# Dashboard\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS DASHBOARD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ranked = sorted(variant_results.items(), key=lambda x: x[1][\"avg\"], reverse=True)\n",
    "for rank, (name, data) in enumerate(ranked, 1):\n",
    "    print(f\"  #{rank} {name}: {data['avg']:.2f}/10\")\n",
    "    score_card(name, data[\"scores\"])\n",
    "\n",
    "print(f\"\\nüèÜ Best variant: {ranked[0][0]} (avg {ranked[0][1]['avg']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Handling Hallucinations and Improving Factuality\n",
    "\n",
    "**Hallucination** occurs when a model confidently generates false or fabricated information. Key strategies to combat it:\n",
    "\n",
    "| Technique | How It Works |\n",
    "|-----------|-------------|\n",
    "| **Uncertainty elicitation** | Ask the model to flag what it's unsure about |\n",
    "| **Grounding / RAG** | Supply source documents; restrict to provided facts |\n",
    "| **Self-verification** | Ask model to check its own answer against its reasoning |\n",
    "| **Citation forcing** | Require inline citations; fabricated citations are detectable |\n",
    "| **Calibration prompts** | Instruct the model to say \"I don't know\" rather than guess |\n",
    "\n",
    "### Experiment 3A: Hallucination-Prone vs. Grounded Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A question designed to elicit hallucination (obscure, specific claim)\n",
    "HALLUCINATION_QUESTION = (\n",
    "    \"What did Dr. Elena Marchetti say about transformer positional encodings \"\n",
    "    \"in her 2021 NeurIPS paper, and what dataset did she use?\"\n",
    ")\n",
    "\n",
    "# ---- PROMPT A: No guardrails (likely to hallucinate) ----\n",
    "print(\"PROMPT A ‚Äî No guardrails (hallucination risk)\")\n",
    "print(\"=\" * 60)\n",
    "msgs_a = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a knowledgeable AI research assistant. Answer questions accurately.\"},\n",
    "    {\"role\": \"user\", \"content\": HALLUCINATION_QUESTION}\n",
    "]\n",
    "show_messages(msgs_a)\n",
    "output_a = chat(msgs_a, temperature=0.5)\n",
    "\n",
    "print()\n",
    "\n",
    "# ---- PROMPT B: With uncertainty elicitation ----\n",
    "print(\"PROMPT B ‚Äî Uncertainty elicitation\")\n",
    "print(\"=\" * 60)\n",
    "msgs_b = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a careful AI research assistant. Important rules:\\n\"\n",
    "            \"- If you are not certain about a specific claim, explicitly say 'I am not certain about this.'\\n\"\n",
    "            \"- If you do not have reliable information, say 'I don't have verified information about this.'\\n\"\n",
    "            \"- Never fabricate names, citations, or specific statistics.\\n\"\n",
    "            \"- Distinguish clearly between what you know with confidence and what is uncertain.\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": HALLUCINATION_QUESTION}\n",
    "]\n",
    "show_messages(msgs_b)\n",
    "output_b = chat(msgs_b, temperature=0.3)\n",
    "\n",
    "print(\"\\nüí° Compare: Prompt A may fabricate details; Prompt B should acknowledge uncertainty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Experiment 3B: Grounded Answering (RAG-Style Constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a RAG scenario: supply a document excerpt, restrict answers to it\n",
    "CONTEXT_DOCUMENT = \"\"\"\n",
    "[SOURCE: TechBrief Q3 Report, 2024]\n",
    "Global smartphone shipments declined 3.2% year-over-year in Q3 2024, reaching 312 million units.\n",
    "Samsung held the largest market share at 22%, followed by Apple at 18%.\n",
    "Xiaomi was the fastest growing brand with 14% YoY growth.\n",
    "5G adoption reached 58% of all new shipments in developed markets.\n",
    "The average selling price (ASP) across all brands rose to $387, up from $361 in Q3 2023.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"What was Apple's market share in Q3 2024?\",          # answerable from context\n",
    "    \"Which brand had the fastest growth?\",                 # answerable\n",
    "    \"What was the global revenue from smartphone sales?\"   # NOT in context ‚Üí should refuse\n",
    "]\n",
    "\n",
    "GROUNDED_SYSTEM = \"\"\"\n",
    "You are a data analyst assistant. You ONLY answer questions based on the provided source document.\n",
    "Rules:\n",
    "- Quote the relevant part of the document when answering.\n",
    "- If the answer is not in the document, say exactly: \"This information is not in the provided source.\"\n",
    "- Never add information from outside the document.\n",
    "- Always cite the source tag at the end of your answer.\n",
    "\"\"\"\n",
    "\n",
    "print(\"GROUNDED ANSWERING ‚Äî Restricted to source document\")\n",
    "print(f\"\\nContext:\\n{CONTEXT_DOCUMENT}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n‚ùì Question: {q}\")\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": GROUNDED_SYSTEM},\n",
    "        {\"role\": \"user\", \"content\": f\"Source document:\\n{CONTEXT_DOCUMENT}\\n\\nQuestion: {q}\"}\n",
    "    ]\n",
    "    _ = chat(msgs, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Experiment 3C: Self-Verification ‚Äî Ask the Model to Check Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_verify(question: str, temperature: float = 0.4):\n",
    "    \"\"\"\n",
    "    Two-stage approach:\n",
    "    1. Generate an initial answer\n",
    "    2. Ask the model to critique and correct its own answer\n",
    "    \"\"\"\n",
    "    # Stage 1: Initial answer\n",
    "    print(\"STAGE 1 ‚Äî Initial answer\")\n",
    "    print(\"-\" * 40)\n",
    "    initial_msgs = [\n",
    "        {\"role\": \"system\", \"content\": \"Answer the question accurately and concisely.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    initial_answer = chat(initial_msgs, temperature=temperature)\n",
    "\n",
    "    # Stage 2: Self-critique\n",
    "    print(\"\\nSTAGE 2 ‚Äî Self-verification\")\n",
    "    print(\"-\" * 40)\n",
    "    verify_msgs = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a fact-checker. Review the answer below critically:\\n\"\n",
    "                \"1. Identify any claims that might be incorrect or that you are not fully certain about.\\n\"\n",
    "                \"2. Flag uncertain claims with [UNCERTAIN].\\n\"\n",
    "                \"3. Correct any errors you find.\\n\"\n",
    "                \"4. Provide a final verified answer.\\n\"\n",
    "                \"Format: Issues Found: [...], Verified Answer: [...]\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question: {question}\\n\\nAnswer to verify:\\n{initial_answer}\"\n",
    "        }\n",
    "    ]\n",
    "    _ = chat(verify_msgs, temperature=0.1)\n",
    "\n",
    "\n",
    "# Test on a question with a commonly confused answer\n",
    "self_verify(\"How many bones does an adult human body have, and which is the smallest?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bias Detection and Mitigation in Prompts\n",
    "\n",
    "Prompts can **introduce or amplify bias** through implicit framing, persona assignment, or loaded language. Mitigation requires:\n",
    "\n",
    "- **Auditing** outputs across demographic and topic dimensions\n",
    "- **Neutral framing** in prompts\n",
    "- **Explicit fairness instructions**\n",
    "- **Perspective balancing** for contested topics\n",
    "\n",
    "### Experiment 4A: Detecting Bias ‚Äî Demographic Consistency Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether the same question yields different tones for different demographics\n",
    "BASE_SCENARIO = \"A candidate applied for a senior software engineering position. They have 5 years of experience and a relevant degree.\"\n",
    "\n",
    "DEMOGRAPHIC_VARIANTS = [\n",
    "    (\"Alex\", \"they/them\"),\n",
    "    (\"Wei Chen\", \"he/him\"),\n",
    "    (\"Fatima Al-Hassan\", \"she/her\"),\n",
    "    (\"James O'Brien\", \"he/him\"),\n",
    "]\n",
    "\n",
    "BIAS_SYSTEM = \"You are a hiring manager. Write a brief (2-sentence) assessment of the candidate's suitability.\"\n",
    "\n",
    "outputs = {}\n",
    "print(\"DEMOGRAPHIC CONSISTENCY TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, pronoun in DEMOGRAPHIC_VARIANTS:\n",
    "    scenario = f\"{BASE_SCENARIO} The candidate's name is {name} ({pronoun}).\"\n",
    "    output = chat(\n",
    "        [{\"role\": \"system\", \"content\": BIAS_SYSTEM}, {\"role\": \"user\", \"content\": scenario}],\n",
    "        show=False, temperature=0.3\n",
    "    )\n",
    "    outputs[name] = output\n",
    "    print(f\"\\n  Candidate: {name} ({pronoun})\")\n",
    "    print(f\"  Assessment: {output}\")\n",
    "\n",
    "# Use the model to detect bias patterns\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BIAS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_outputs = \"\\n\\n\".join(f\"{name}: {out}\" for name, out in outputs.items())\n",
    "bias_analysis_msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a bias auditor. Analyse the following hiring assessments for the same-qualified candidate \"\n",
    "            \"with different names/demographics. Identify:\\n\"\n",
    "            \"1. Any differences in tone, language strength, or assumptions\\n\"\n",
    "            \"2. Whether any candidate received notably warmer or colder language\\n\"\n",
    "            \"3. Specific biased phrases or patterns, if any\\n\"\n",
    "            \"4. Overall bias verdict: LOW / MODERATE / HIGH\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"Assessments for equally qualified candidates:\\n\\n{all_outputs}\"}\n",
    "]\n",
    "_ = chat(bias_analysis_msgs, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Experiment 4B: Mitigation ‚Äî Explicit Fairness Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run with explicit fairness constraints\n",
    "FAIR_SYSTEM = \"\"\"\n",
    "You are a hiring manager committed to fair and equitable evaluation.\n",
    "Rules:\n",
    "- Evaluate ONLY based on the stated qualifications (experience and degree).\n",
    "- Do NOT make assumptions based on the candidate's name or demographic markers.\n",
    "- Use identical assessment criteria and language strength for all candidates.\n",
    "- Write a 2-sentence suitability assessment that would be identical in substance\n",
    "  regardless of the candidate's name, gender, or cultural background.\n",
    "\"\"\"\n",
    "\n",
    "fair_outputs = {}\n",
    "print(\"BIAS MITIGATION ‚Äî With Explicit Fairness Instructions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, pronoun in DEMOGRAPHIC_VARIANTS:\n",
    "    scenario = f\"{BASE_SCENARIO} The candidate's name is {name} ({pronoun}).\"\n",
    "    output = chat(\n",
    "        [{\"role\": \"system\", \"content\": FAIR_SYSTEM}, {\"role\": \"user\", \"content\": scenario}],\n",
    "        show=False, temperature=0.3\n",
    "    )\n",
    "    fair_outputs[name] = output\n",
    "    print(f\"\\n  Candidate: {name}\")\n",
    "    print(f\"  Assessment: {output}\")\n",
    "\n",
    "# Compare bias levels\n",
    "fair_all = \"\\n\\n\".join(f\"{name}: {out}\" for name, out in fair_outputs.items())\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RE-AUDIT after mitigation\")\n",
    "print(\"=\" * 60)\n",
    "_ = chat([\n",
    "    {\"role\": \"system\", \"content\": bias_analysis_msgs[0][\"content\"]},\n",
    "    {\"role\": \"user\", \"content\": f\"Assessments (after fairness prompt):\\n\\n{fair_all}\"}\n",
    "], temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Experiment 4C: Perspective Balancing for Contested Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTESTED_TOPIC = \"Should companies mandate a return to office for all employees?\"\n",
    "\n",
    "# ---- Biased framing (implicitly one-sided) ----\n",
    "print(\"BIASED FRAMING\")\n",
    "print(\"=\" * 60)\n",
    "biased_msgs = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a productivity consultant who believes in-person collaboration is essential.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{CONTESTED_TOPIC} Give your view.\"}\n",
    "]\n",
    "show_messages(biased_msgs)\n",
    "_ = chat(biased_msgs, temperature=0.4)\n",
    "\n",
    "print()\n",
    "\n",
    "# ---- Balanced framing ----\n",
    "print(\"BALANCED FRAMING\")\n",
    "print(\"=\" * 60)\n",
    "balanced_msgs = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a neutral organisational analyst. When discussing contested workplace topics:\\n\"\n",
    "            \"1. Present the strongest arguments FOR (2 bullets)\\n\"\n",
    "            \"2. Present the strongest arguments AGAINST (2 bullets)\\n\"\n",
    "            \"3. Identify the key factor that determines which side is right for a given company\\n\"\n",
    "            \"4. Do NOT state a personal preference or conclusion.\\n\"\n",
    "            \"Label sections clearly: Arguments For / Arguments Against / Key Decision Factor.\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": CONTESTED_TOPIC}\n",
    "]\n",
    "show_messages(balanced_msgs)\n",
    "_ = chat(balanced_msgs, temperature=0.3)\n",
    "\n",
    "print(\"\\nüí° Balanced framing lets readers draw their own informed conclusions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Documentation and Version Control for Prompts\n",
    "\n",
    "Prompts are **first-class production assets** and need the same rigor as code:\n",
    "\n",
    "- Unique IDs and semantic versioning (`v1.0.0`)\n",
    "- Metadata: author, date, task, model target\n",
    "- Changelog entries explaining *why* each version changed\n",
    "- Test cases + expected outputs baked in\n",
    "- A registry for discovery and retrieval\n",
    "\n",
    "### Experiment 5A: Prompt Schema and Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  PROMPT REGISTRY ‚Äî a simple in-memory versioned store\n",
    "# ============================================================\n",
    "\n",
    "class PromptVersion:\n",
    "    \"\"\"A single versioned snapshot of a prompt.\"\"\"\n",
    "\n",
    "    def __init__(self, version: str, system: str, user_template: str,\n",
    "                 author: str, change_reason: str, test_cases: list = None):\n",
    "        self.id = str(uuid.uuid4())[:8]\n",
    "        self.version = version\n",
    "        self.system = system\n",
    "        self.user_template = user_template\n",
    "        self.author = author\n",
    "        self.created_at = datetime.now(timezone.utc).isoformat()\n",
    "        self.change_reason = change_reason\n",
    "        self.test_cases = test_cases or []\n",
    "        # Fingerprint for change detection\n",
    "        content = system + user_template\n",
    "        self.fingerprint = hashlib.sha256(content.encode()).hexdigest()[:12]\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"version\": self.version,\n",
    "            \"fingerprint\": self.fingerprint,\n",
    "            \"author\": self.author,\n",
    "            \"created_at\": self.created_at,\n",
    "            \"change_reason\": self.change_reason,\n",
    "            \"system_preview\": self.system[:80] + \"...\",\n",
    "            \"test_cases_count\": len(self.test_cases),\n",
    "        }\n",
    "\n",
    "\n",
    "class PromptRegistry:\n",
    "    \"\"\"Registry for storing and retrieving versioned prompts.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._store: dict[str, list[PromptVersion]] = defaultdict(list)\n",
    "\n",
    "    def register(self, prompt_name: str, version: PromptVersion):\n",
    "        self._store[prompt_name].append(version)\n",
    "        print(f\"‚úÖ Registered '{prompt_name}' v{version.version} [id:{version.id}]\")\n",
    "\n",
    "    def get_latest(self, prompt_name: str) -> PromptVersion:\n",
    "        versions = self._store.get(prompt_name, [])\n",
    "        return versions[-1] if versions else None\n",
    "\n",
    "    def get_version(self, prompt_name: str, version: str) -> PromptVersion:\n",
    "        return next((v for v in self._store.get(prompt_name, []) if v.version == version), None)\n",
    "\n",
    "    def list_versions(self, prompt_name: str):\n",
    "        \"\"\"Print a changelog view.\"\"\"\n",
    "        versions = self._store.get(prompt_name, [])\n",
    "        print(f\"\\nüìã Changelog for '{prompt_name}':\")\n",
    "        for v in versions:\n",
    "            print(f\"  v{v.version} [{v.id}] | {v.created_at[:10]} | {v.author}\")\n",
    "            print(f\"    Reason: {v.change_reason}\")\n",
    "            print(f\"    Fingerprint: {v.fingerprint}\")\n",
    "\n",
    "    def diff(self, prompt_name: str, v1: str, v2: str):\n",
    "        \"\"\"Show system prompt diff between two versions.\"\"\"\n",
    "        pv1 = self.get_version(prompt_name, v1)\n",
    "        pv2 = self.get_version(prompt_name, v2)\n",
    "        if not pv1 or not pv2:\n",
    "            print(\"One or both versions not found.\")\n",
    "            return\n",
    "        print(f\"\\nüîç Diff: {prompt_name} v{v1} ‚Üí v{v2}\")\n",
    "        print(f\"  Fingerprint changed: {pv1.fingerprint} ‚Üí {pv2.fingerprint}\")\n",
    "        s1_lines = pv1.system.splitlines()\n",
    "        s2_lines = pv2.system.splitlines()\n",
    "        removed = set(s1_lines) - set(s2_lines)\n",
    "        added = set(s2_lines) - set(s1_lines)\n",
    "        for line in removed:\n",
    "            if line.strip():\n",
    "                print(f\"  - {line}\")\n",
    "        for line in added:\n",
    "            if line.strip():\n",
    "                print(f\"  + {line}\")\n",
    "\n",
    "\n",
    "# ---- Populate the registry with versions of a real prompt ----\n",
    "registry = PromptRegistry()\n",
    "\n",
    "# v1.0.0 ‚Äî initial\n",
    "registry.register(\"email-support\", PromptVersion(\n",
    "    version=\"1.0.0\",\n",
    "    system=\"You are a customer support agent. Reply to customer emails helpfully.\",\n",
    "    user_template=\"Customer email:\\n{email}\",\n",
    "    author=\"alice@team.com\",\n",
    "    change_reason=\"Initial release\",\n",
    "    test_cases=[\n",
    "        {\"input\": \"My order hasn't arrived.\", \"expected_keywords\": [\"apologise\", \"check\", \"order\"]}\n",
    "    ]\n",
    "))\n",
    "\n",
    "# v1.1.0 ‚Äî added tone guidance after negative user feedback\n",
    "registry.register(\"email-support\", PromptVersion(\n",
    "    version=\"1.1.0\",\n",
    "    system=(\n",
    "        \"You are a senior customer support specialist. Reply to emails with empathy and clarity.\\n\"\n",
    "        \"Always: acknowledge the issue, offer a resolution, set timeframe expectations.\"\n",
    "    ),\n",
    "    user_template=\"Customer email:\\n{email}\",\n",
    "    author=\"bob@team.com\",\n",
    "    change_reason=\"Added empathy and structured response guidelines after CSAT drop in week 12\",\n",
    "    test_cases=[\n",
    "        {\"input\": \"My order hasn't arrived.\", \"expected_keywords\": [\"apologise\", \"check\", \"within\"]},\n",
    "        {\"input\": \"I was charged twice!\", \"expected_keywords\": [\"refund\", \"business days\"]}\n",
    "    ]\n",
    "))\n",
    "\n",
    "# v2.0.0 ‚Äî major rewrite with length constraint and routing hint\n",
    "registry.register(\"email-support\", PromptVersion(\n",
    "    version=\"2.0.0\",\n",
    "    system=(\n",
    "        \"You are a senior customer support specialist at a SaaS company.\\n\"\n",
    "        \"Rules: Acknowledge issue in sentence 1. Provide resolution in sentence 2-3. \"\n",
    "        \"Set expectations (timeframe). Close warmly. Under 120 words. Plain English only.\"\n",
    "    ),\n",
    "    user_template=\"Customer email:\\n{email}\\n\\nCustomer tier: {tier}\",\n",
    "    author=\"alice@team.com\",\n",
    "    change_reason=\"Major rewrite: added word limit, customer tier variable, plain English rule. v2 tested 8.4 avg CSAT vs 7.1 in v1.1\",\n",
    "    test_cases=[\n",
    "        {\"input\": {\"email\": \"I can't login\", \"tier\": \"Enterprise\"}, \"expected_keywords\": [\"escalate\", \"priority\"]}\n",
    "    ]\n",
    "))\n",
    "\n",
    "# View changelog\n",
    "registry.list_versions(\"email-support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "### Experiment 5B: Diff Versions and Run Regression Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show diff between two versions\n",
    "registry.diff(\"email-support\", \"1.0.0\", \"2.0.0\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ---- REGRESSION TEST: run all test cases on each version ----\n",
    "def run_regression(prompt_name: str, reg: PromptRegistry):\n",
    "    \"\"\"Run test cases for all versions and report pass/fail.\"\"\"\n",
    "    versions = reg._store.get(prompt_name, [])\n",
    "    print(f\"\\nüß™ REGRESSION TESTS for '{prompt_name}'\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for pv in versions:\n",
    "        if not pv.test_cases:\n",
    "            continue\n",
    "        print(f\"\\n  v{pv.version} ‚Äî {len(pv.test_cases)} test(s)\")\n",
    "        for i, tc in enumerate(pv.test_cases):\n",
    "            user_input = tc[\"input\"]\n",
    "            keywords = tc.get(\"expected_keywords\", [])\n",
    "\n",
    "            # Format user message (handle dict or str inputs)\n",
    "            if isinstance(user_input, dict):\n",
    "                formatted = pv.user_template\n",
    "                for k, v in user_input.items():\n",
    "                    formatted = formatted.replace(f\"{{{k}}}\", str(v))\n",
    "            else:\n",
    "                formatted = pv.user_template.replace(\"{email}\", user_input).replace(\"{tier}\", \"Standard\")\n",
    "\n",
    "            output = chat(\n",
    "                [{\"role\": \"system\", \"content\": pv.system}, {\"role\": \"user\", \"content\": formatted}],\n",
    "                show=False, temperature=0.1\n",
    "            )\n",
    "\n",
    "            hits = [kw for kw in keywords if kw.lower() in output.lower()]\n",
    "            status = \"‚úÖ PASS\" if len(hits) == len(keywords) else f\"‚ùå FAIL (missing: {set(keywords)-set(hits)})\"\n",
    "            print(f\"    Test {i+1}: {status}\")\n",
    "            print(f\"      Input: {str(user_input)[:60]}\")\n",
    "            print(f\"      Output: {output[:120]}...\")\n",
    "\n",
    "\n",
    "run_regression(\"email-support\", registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "### Experiment 5C: Auto-Document a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_document_prompt(system_prompt: str, user_template: str) -> str:\n",
    "    \"\"\"Use the LLM to auto-generate documentation for a prompt.\"\"\"\n",
    "    doc_msgs = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a technical writer specialising in AI prompt documentation. \"\n",
    "                \"Generate a concise markdown doc block for the given prompt. Include:\\n\"\n",
    "                \"- **Purpose**: What task this prompt accomplishes\\n\"\n",
    "                \"- **Input Variables**: List of {placeholders} and what they represent\\n\"\n",
    "                \"- **Output Format**: What the model returns\\n\"\n",
    "                \"- **Constraints**: Key rules or guardrails in the prompt\\n\"\n",
    "                \"- **Best For**: Ideal use cases\\n\"\n",
    "                \"- **Not Suitable For**: When NOT to use this prompt\\n\"\n",
    "                \"- **Suggested Model**: Recommended model tier (small/medium/large)\\n\"\n",
    "                \"Keep the total doc under 200 words.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"System prompt:\\n```\\n{system_prompt}\\n```\\n\\n\"\n",
    "                f\"User template:\\n```\\n{user_template}\\n```\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    return chat(doc_msgs, temperature=0.3)\n",
    "\n",
    "\n",
    "# Document the latest version of our email-support prompt\n",
    "latest = registry.get_latest(\"email-support\")\n",
    "print(f\"AUTO-DOCUMENTING: email-support v{latest.version}\")\n",
    "print(\"=\" * 60)\n",
    "doc = auto_document_prompt(latest.system, latest.user_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Experiment 5D: Export Registry to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_registry(reg: PromptRegistry) -> dict:\n",
    "    \"\"\"Export the full registry to a JSON-serialisable dict.\"\"\"\n",
    "    export = {}\n",
    "    for name, versions in reg._store.items():\n",
    "        export[name] = {\n",
    "            \"latest_version\": versions[-1].version if versions else None,\n",
    "            \"total_versions\": len(versions),\n",
    "            \"versions\": [v.to_dict() for v in versions]\n",
    "        }\n",
    "    return export\n",
    "\n",
    "\n",
    "registry_export = export_registry(registry)\n",
    "print(\"REGISTRY EXPORT (JSON)\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(registry_export, indent=2))\n",
    "\n",
    "print(\"\\nüí° Save this JSON to disk or a database to persist your prompt history across sessions.\")\n",
    "print(\"   Load it back by re-instantiating PromptVersion objects from the stored data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Sandbox ‚Äî Try It Yourself!\n",
    "\n",
    "Experiment freely with any technique from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  SANDBOX\n",
    "# ============================================================\n",
    "\n",
    "# ---- Try: Iterative Refinement ----\n",
    "# final = auto_refine(\n",
    "#     task_description=\"Your task here\",\n",
    "#     initial_prompt=\"Your weak starting prompt\",\n",
    "#     test_input=\"A sample input\",\n",
    "#     criteria=[\"criterion1\", \"criterion2\"],\n",
    "#     target_score=8.0\n",
    "# )\n",
    "\n",
    "# ---- Try: A/B test your own variants ----\n",
    "# VARIANTS = [{\"name\": \"A\", \"system\": \"...\"}, {\"name\": \"B\", \"system\": \"...\"}]\n",
    "# ... run and score\n",
    "\n",
    "# ---- Try: Self-verification ----\n",
    "# self_verify(\"What is the capital of Australia, and what is its population?\")\n",
    "\n",
    "# ---- Try: Register your own prompt version ----\n",
    "# registry.register(\"my-prompt\", PromptVersion(\n",
    "#     version=\"1.0.0\",\n",
    "#     system=\"Your system prompt here\",\n",
    "#     user_template=\"User input: {input}\",\n",
    "#     author=\"your@email.com\",\n",
    "#     change_reason=\"Initial version\"\n",
    "# ))\n",
    "\n",
    "# Default sandbox: test the grounded answering technique on your own document\n",
    "MY_DOCUMENT = \"\"\"\n",
    "[SOURCE: My Notes]\n",
    "The playground notebook covers five topics: iterative refinement, A/B testing,\n",
    "hallucination handling, bias detection, and prompt documentation.\n",
    "It uses the gpt-oss:20b-cloud model via ChatOllama.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": GROUNDED_SYSTEM},\n",
    "    {\"role\": \"user\", \"content\": f\"Source:\\n{MY_DOCUMENT}\\n\\nQuestion: What model does the notebook use?\"}\n",
    "]\n",
    "\n",
    "print(\"SANDBOX ‚Äî Grounded answer from custom document\")\n",
    "print(\"=\" * 60)\n",
    "show_messages(messages)\n",
    "_ = chat(messages, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Best Practice | Core Principle | Key Tool/Pattern |\n",
    "|---------------|---------------|------------------|\n",
    "| **Iterative Refinement** | Treat prompts as hypotheses; test and improve empirically | Score ‚Üí Critique ‚Üí Rewrite loop |\n",
    "| **A/B Testing** | Let metrics decide, not intuition | Judge LLM + scoring rubric |\n",
    "| **Hallucination Handling** | Ground answers; encourage \"I don't know\" | Context constraints + self-verification |\n",
    "| **Bias Mitigation** | Test across demographics; add explicit fairness rules | Consistency audit + balanced framing |\n",
    "| **Documentation & Versioning** | Prompts are code ‚Äî track changes, reasons, and tests | PromptRegistry + semantic versioning |\n",
    "\n",
    "### Prompt Quality Checklist\n",
    "\n",
    "Before deploying a prompt to production:\n",
    "\n",
    "```\n",
    "‚òê Baseline scored against criteria?\n",
    "‚òê At least one round of refinement completed?\n",
    "‚òê A/B tested against at least one alternative?\n",
    "‚òê Tested on edge cases and adversarial inputs?\n",
    "‚òê Hallucination guardrails in place?\n",
    "‚òê Bias audit run across demographic variants?\n",
    "‚òê Registered with version, author, and change reason?\n",
    "‚òê Test cases written and passing?\n",
    "‚òê Auto-documentation generated?\n",
    "```\n",
    "\n",
    "### The Prompt Lifecycle\n",
    "\n",
    "```\n",
    "Draft ‚îÄ‚îÄ‚ñ∂ Score ‚îÄ‚îÄ‚ñ∂ Critique ‚îÄ‚îÄ‚ñ∂ Refine\n",
    "                                    ‚îÇ\n",
    "                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      ‚ñº\n",
    "               A/B Test ‚îÄ‚îÄ‚ñ∂ Bias Audit ‚îÄ‚îÄ‚ñ∂ Hallucination Check\n",
    "                                                   ‚îÇ\n",
    "                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                               ‚ñº\n",
    "                     Register v1.0.0 ‚îÄ‚îÄ‚ñ∂ Deploy ‚îÄ‚îÄ‚ñ∂ Monitor\n",
    "                                                       ‚îÇ\n",
    "                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                          ‚ñº\n",
    "                                  v1.1.0 (iterate)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
