{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 LLM Generation Parameters\n",
    "\n",
    "## Playground Notebook\n",
    "\n",
    "Generation parameters are the **knobs and dials** that control how a model produces text. They don't change *what* the model knows ‚Äî they change *how* it samples from its knowledge.\n",
    "\n",
    "| Parameter | What It Controls |\n",
    "|-----------|------------------|\n",
    "| **Temperature** | Randomness and creativity in outputs |\n",
    "| **Top-P** | Limits token selection to a cumulative probability threshold |\n",
    "| **Top-K** | Limits sampling to only the K most likely tokens |\n",
    "| **Max Tokens** | Maximum number of tokens the model can generate |\n",
    "| **Frequency Penalty** | Penalizes tokens based on how often they've appeared |\n",
    "| **Presence Penalty** | Penalizes any token that has appeared at least once |\n",
    "| **Stop Sequences** | Strings that immediately halt generation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# ============================================================\n",
    "#  CONFIGURATION - Change the model name here if needed\n",
    "# ============================================================\n",
    "MODEL = \"qwen2.5:1.5b\"  # Options: \"qwen2.5:1.5b\", \"llama3.2\", \"mistral\", \"gemma2\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using model: qwen2.5:1.5b\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def generate(prompt, system=\"You are a helpful assistant.\", **kwargs):\n",
    "    \"\"\"Send a prompt with custom generation parameters and return the response.\"\"\"\n",
    "    llm = ChatOllama(model=MODEL, **kwargs)\n",
    "    messages = [SystemMessage(content=system), HumanMessage(content=prompt)]\n",
    "    start = time.time()\n",
    "    response = llm.invoke(messages)\n",
    "    elapsed = time.time() - start\n",
    "    content = response.content\n",
    "    display(Markdown(content))\n",
    "    print(f\"\\n‚è±Ô∏è {elapsed:.2f}s | {len(content)} chars\")\n",
    "    return content\n",
    "\n",
    "\n",
    "def compare(prompt, configs, system=\"You are a helpful assistant.\"):\n",
    "    \"\"\"Run the same prompt with different parameter configs side by side.\"\"\"\n",
    "    results = {}\n",
    "    for cfg in configs:\n",
    "        label = cfg.pop(\"label\")\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"  {label}\")\n",
    "        params_str = ', '.join(f'{k}={v}' for k, v in cfg.items())\n",
    "        print(f\"  Parameters: {params_str}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        results[label] = generate(prompt, system=system, **cfg)\n",
    "        cfg[\"label\"] = label  # restore label\n",
    "    return results\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Temperature ‚Äî Controlling Randomness\n",
    "\n",
    "Temperature scales the probability distribution before sampling:\n",
    "\n",
    "```\n",
    "Temperature = 0.0  ‚Üí  Always pick the most likely token (deterministic)\n",
    "Temperature = 0.7  ‚Üí  Balanced creativity (good default)\n",
    "Temperature = 1.5  ‚Üí  Very random, creative, sometimes incoherent\n",
    "```\n",
    "\n",
    "**Think of it like a dial:**\n",
    "```\n",
    "FOCUSED ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ CREATIVE\n",
    "  0.0       0.3    0.7    1.0      1.5+\n",
    "```\n",
    "\n",
    "### Experiment 1A: Low vs. Medium vs. High Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  üßä Temperature = 0.0 (Deterministic)\n",
      "  Parameters: temperature=0.0\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The vast and mysterious ocean covers approximately 71% of Earth's surface and is home to an incredible array of life forms, from tiny plankton to massive whales."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 1.90s | 161 chars\n",
      "\n",
      "============================================================\n",
      "  ‚öñÔ∏è Temperature = 0.7 (Balanced)\n",
      "  Parameters: temperature=0.7\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The vast and mysterious ocean covers more than 70% of our planet's surface and is home to an incredible array of life forms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.69s | 124 chars\n",
      "\n",
      "============================================================\n",
      "  üî• Temperature = 1.5 (Very Creative)\n",
      "  Parameters: temperature=1.5\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The vast,ËîöËìù„ÄÅÊ≥¢ÊæúÂ£ÆÈòîÁöÑÊµ∑Ê¥ãÔºåÊòØÂú∞ÁêÉ‰∏äÁîüÂëΩÁöÑÈáçË¶ÅÊ∫êÊ≥â„ÄÇ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.58s | 32 chars\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a one-sentence description of the ocean.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\": \"üßä Temperature = 0.0 (Deterministic)\", \"temperature\": 0.0},\n",
    "    {\"label\": \"‚öñÔ∏è Temperature = 0.7 (Balanced)\",     \"temperature\": 0.7},\n",
    "    {\"label\": \"üî• Temperature = 1.5 (Very Creative)\", \"temperature\": 1.5},\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1B: Temperature & Consistency\n",
    "\n",
    "At **temperature 0**, the model should produce the *same* output every time. At higher temperatures, each run differs. Let's verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Temperature = 0.0 ‚Äî Running 4 times\n",
      "============================================================\n",
      "  Run 1: Blue is one of the colors.\n",
      "  Run 2: Blue is one of the colors.\n",
      "  Run 3: Blue is one of the colors.\n",
      "  Run 4: Blue is one of the colors.\n",
      "  ‚Üí Unique outputs: 1/4\n",
      "\n",
      "============================================================\n",
      "  Temperature = 1.0 ‚Äî Running 4 times\n",
      "============================================================\n",
      "  Run 1: I'll choose blue as the color to name.\n",
      "  Run 2: Sure! Here's a random choice: **Blue**. Blue is the color associated with calmne\n",
      "  Run 3: Blue is one of the colors.\n",
      "  Run 4: Blue is a color.\n",
      "  ‚Üí Unique outputs: 4/4\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Name one color.\"\n",
    "num_runs = 4\n",
    "\n",
    "for temp in [0.0, 1.0]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  Temperature = {temp} ‚Äî Running {num_runs} times\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    outputs = []\n",
    "    for i in range(num_runs):\n",
    "        llm = ChatOllama(model=MODEL, temperature=temp)\n",
    "        resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "        text = resp.content.strip()\n",
    "        outputs.append(text)\n",
    "        print(f\"  Run {i+1}: {text[:80]}\")\n",
    "    unique = len(set(outputs))\n",
    "    print(f\"  ‚Üí Unique outputs: {unique}/{num_runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1C: Temperature for Different Tasks\n",
    "\n",
    "Different tasks need different temperature settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "  TASK: Factual Q&A (low temp is better)\n",
      "############################################################\n",
      "\n",
      "--- Temperature = 0.0 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The boiling point of water at standard atmospheric pressure (1 atmosphere) is 100 degrees Celsius."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.69s | 98 chars\n",
      "\n",
      "--- Temperature = 0.7 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The boiling point of water at standard atmospheric pressure (1 atmosphere) is 100 degrees Celsius."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.60s | 98 chars\n",
      "\n",
      "--- Temperature = 1.5 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The boiling point of water at standard atmospheric pressure (1 atmosphere or 760 mmHg) is 100¬∞Celsius."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.97s | 102 chars\n",
      "\n",
      "üí° Observation: Compare how temperature affects accuracy vs. creativity above.\n",
      "\n",
      "############################################################\n",
      "  TASK: Creative Writing (higher temp is better)\n",
      "############################################################\n",
      "\n",
      "--- Temperature = 0.0 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A gentle wave of orange and pink hues crept across the horizon, painting the twilight sky with its final strokes before night's embrace."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 1.72s | 136 chars\n",
      "\n",
      "--- Temperature = 0.7 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "As twilight approached, the colors of the setting sun painted the horizon in vivid strokes of orange and pink, while the mountains below were bathed in a gentle warmth that slowly faded into night."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 2.32s | 197 chars\n",
      "\n",
      "--- Temperature = 1.5 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "A golden orb descends, earthward, in gentle surrender, painting vast lands with hues of twilight's most exquisite palette: pinks, purples, and grays intertwine into serene vistas that seem to hold a cosmic whisper."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 2.92s | 214 chars\n",
      "\n",
      "üí° Observation: Compare how temperature affects accuracy vs. creativity above.\n",
      "\n",
      "############################################################\n",
      "  TASK: Code Generation (low temp is better)\n",
      "############################################################\n",
      "\n",
      "--- Temperature = 0.0 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a Python one-liner to reverse a string:\n",
       "\n",
       "```python\n",
       "reverse_string = lambda s: s[::-1]\n",
       "```\n",
       "\n",
       "This uses the `s[::-1]` syntax, which is a built-in slice notation in Python that returns a reversed copy of the string `s`. The `lambda` function creates an anonymous function that takes a single argument `s`, and returns its reverse."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 4.90s | 333 chars\n",
      "\n",
      "--- Temperature = 0.7 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a one-liner in Python to reverse a string:\n",
       "\n",
       "```python\n",
       "reversed_string = ''.join(reversed(string))\n",
       "```\n",
       "\n",
       "This line of code takes the input string, `string`, and uses the built-in `reversed()` function to reverse it character by character. The result is then joined back into a single string using the `join()` method with an empty string as the separator.\n",
       "\n",
       "Note that this one-liner only works if you're working in Python 3.x where strings are Unicode, or if you want to explicitly cast the input to a string type (e.g., `str(input())`)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 9.47s | 541 chars\n",
      "\n",
      "--- Temperature = 1.5 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is a one-line Python solution using slicing:\n",
       "\n",
       "```python\n",
       "reversed_string = s[::-1]\n",
       "```\n",
       "\n",
       "However, note that the slicing syntax `[::-1]` creates a copy of the original string in reverse order, so this is just a view and not an actual change to the original string. If you want to modify the original string, you can assign it back to itself:\n",
       "\n",
       "```python\n",
       "s = \"Hello, World!\"\n",
       "reversed_string = s[::-1]\n",
       "```\n",
       "\n",
       "Alternatively, you could use Python's `reversed` function directly on the string:\n",
       "\n",
       "```python\n",
       "reversed_string = \"\".join(reversed(\"Hello, World!\"))\n",
       "```\n",
       "\n",
       "However, note that this method also returns a new reversed version of the original string, so it is only for assignment and not modification."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 15.98s | 698 chars\n",
      "\n",
      "üí° Observation: Compare how temperature affects accuracy vs. creativity above.\n"
     ]
    }
   ],
   "source": [
    "tasks = [\n",
    "    {\n",
    "        \"label\": \"Factual Q&A (low temp is better)\",\n",
    "        \"prompt\": \"What is the capital of Inida? Answer in one sentence.\",\n",
    "        \"temps\": [0.0, 0.7, 1.5]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Creative Writing (higher temp is better)\",\n",
    "        \"prompt\": \"Describe a sunset without using the word 'sun' or 'sky'. One sentence only.\",\n",
    "        \"temps\": [0.0, 0.7, 1.5]\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"Code Generation (low temp is better)\",\n",
    "        \"prompt\": \"Write a Python one-liner that reverses a string.\",\n",
    "        \"temps\": [0.0, 0.7, 1.5]\n",
    "    }\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    print(f\"\\n{'#' * 60}\")\n",
    "    print(f\"  TASK: {task['label']}\")\n",
    "    print(f\"{'#' * 60}\")\n",
    "    for temp in task[\"temps\"]:\n",
    "        print(f\"\\n--- Temperature = {temp} ---\")\n",
    "        _ = generate(task[\"prompt\"], temperature=temp)\n",
    "\n",
    "    print(f\"\\nüí° Observation: Compare how temperature affects accuracy vs. creativity above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Top-P (Nucleus Sampling) ‚Äî Probability Threshold\n",
    "\n",
    "Top-P limits the model to the **smallest set of tokens** whose cumulative probability adds up to P.\n",
    "\n",
    "```\n",
    "Top-P = 0.1  ‚Üí  Only the top ~10% probability mass (very focused)\n",
    "Top-P = 0.9  ‚Üí  Top ~90% probability mass (more diverse)\n",
    "Top-P = 1.0  ‚Üí  Consider all tokens (no filtering)\n",
    "```\n",
    "\n",
    "**Example ‚Äî Next token probabilities:**\n",
    "```\n",
    "Token:   \"the\"   \"a\"    \"an\"   \"one\"  \"my\"  \"some\" ...\n",
    "Prob:     0.35   0.25   0.15   0.10   0.05   0.03  ...\n",
    "\n",
    "Top-P=0.5 ‚Üí selects {\"the\", \"a\"} (0.35+0.25=0.60 ‚â• 0.5)\n",
    "Top-P=0.9 ‚Üí selects {\"the\", \"a\", \"an\", \"one\", \"my\"}\n",
    "```\n",
    "\n",
    "### Experiment 2A: Top-P Narrow vs. Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  üéØ Top-P = 0.1 (Very Focused)\n",
      "  Parameters: top_p=0.1, temperature=0.8\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. Playing the banjo: This instrument is often associated with country music and bluegrass, but it can also be played as an unusual hobby.\n",
       "2. Collecting rare stamps: Many people collect stamps for fun or to add to their collections of memorabilia.\n",
       "3. Growing a garden: While many hobbies involve growing plants, some individuals enjoy the challenge of growing unique or exotic species that are not commonly found in local gardens.\n",
       "4. Playing chess: Chess is often considered a boring hobby, but it can be enjoyed by those who find it challenging and intellectually stimulating.\n",
       "5. Collecting vintage toys: Some people collect old toys as a way to preserve history and add nostalgia to their homes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 3.30s | 697 chars\n",
      "\n",
      "============================================================\n",
      "  ‚öñÔ∏è Top-P = 0.5 (Moderate)\n",
      "  Parameters: top_p=0.5, temperature=0.8\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. Playing with slime: Some people find it fun to create and play with slimy substances.\n",
       "2. Rock climbing: This adventurous hobby involves scaling steep rock faces, often outdoors or in indoor climbing walls.\n",
       "3. Making jewelry from recycled materials: Instead of using new metals and gemstones, some individuals collect old items like broken glass, buttons, and metal scraps and transform them into beautiful jewelry pieces.\n",
       "4. Growing a garden with unusual plants: Some people enjoy growing rare or exotic plants that are not commonly found in their area.\n",
       "5. Building model airplanes: This hobby involves creating miniature versions of real aircraft using plastic, wood, and other materials."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 2.18s | 692 chars\n",
      "\n",
      "============================================================\n",
      "  üåä Top-P = 0.95 (Diverse)\n",
      "  Parameters: top_p=0.95, temperature=0.8\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. Collecting rare stamps or coins.\n",
       "2. Playing Dungeons and Dragons or other tabletop role-playing games.\n",
       "3. Learning to juggle or perform acrobatics for entertainment.\n",
       "4. Growing exotic houseplants that require specific care and attention.\n",
       "5. Knitting or crocheting clothes, accessories, or blankets that are both functional and beautiful."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 1.40s | 340 chars\n"
     ]
    }
   ],
   "source": [
    "prompt = \"List 5 unusual hobbies someone might enjoy.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\": \"üéØ Top-P = 0.1 (Very Focused)\",  \"top_p\": 0.1, \"temperature\": 0.8},\n",
    "    {\"label\": \"‚öñÔ∏è Top-P = 0.5 (Moderate)\",       \"top_p\": 0.5, \"temperature\": 0.8},\n",
    "    {\"label\": \"üåä Top-P = 0.95 (Diverse)\",       \"top_p\": 0.95, \"temperature\": 0.8},\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2B: Temperature vs. Top-P ‚Äî They Work Together\n",
    "\n",
    "Temperature reshapes probabilities *first*, then Top-P filters the result. Using both together gives fine-grained control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Low Temp + Low Top-P (Most predictable)\n",
      "  Parameters: temperature=0.2, top_p=0.3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Drinking Springs\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.49s | 18 chars\n",
      "\n",
      "============================================================\n",
      "  Low Temp + High Top-P\n",
      "  Parameters: temperature=0.2, top_p=0.95\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Feastfire's Forge\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.30s | 19 chars\n",
      "\n",
      "============================================================\n",
      "  High Temp + Low Top-P\n",
      "  Parameters: temperature=1.2, top_p=0.3\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"Druid's Den\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.20s | 13 chars\n",
      "\n",
      "============================================================\n",
      "  High Temp + High Top-P (Most creative)\n",
      "  Parameters: temperature=1.2, top_p=0.95\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\"The Shadowy Vale\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.20s | 18 chars\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Invent a name for a fantasy tavern.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\": \"Low Temp + Low Top-P (Most predictable)\",   \"temperature\": 0.2, \"top_p\": 0.3},\n",
    "    {\"label\": \"Low Temp + High Top-P\",                     \"temperature\": 0.2, \"top_p\": 0.95},\n",
    "    {\"label\": \"High Temp + Low Top-P\",                     \"temperature\": 1.2, \"top_p\": 0.3},\n",
    "    {\"label\": \"High Temp + High Top-P (Most creative)\",    \"temperature\": 1.2, \"top_p\": 0.95},\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Top-K ‚Äî Fixed Token Pool Size\n",
    "\n",
    "Top-K is simpler than Top-P: it always considers exactly the **K most likely tokens**, regardless of their probabilities.\n",
    "\n",
    "```\n",
    "Top-K = 1   ‚Üí  Greedy decoding (always pick the #1 token)\n",
    "Top-K = 10  ‚Üí  Choose from top 10 tokens\n",
    "Top-K = 50  ‚Üí  Choose from top 50 tokens (more variety)\n",
    "```\n",
    "\n",
    "### Experiment 3A: Top-K Values Compared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Top-K = 1 (Greedy ‚Äî always picks top token)\n",
      "  Parameters: top_k=1, temperature=0.8\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Job"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 0.77s | 3 chars\n",
      "\n",
      "============================================================\n",
      "  Top-K = 5\n",
      "  Parameters: top_k=5, temperature=0.8\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Task."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 3.22s | 5 chars\n",
      "\n",
      "============================================================\n",
      "  Top-K = 40 (Default for many models)\n",
      "  Parameters: top_k=40, temperature=0.8\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Job"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 3.42s | 3 chars\n",
      "\n",
      "============================================================\n",
      "  Top-K = 100 (Wide pool)\n",
      "  Parameters: top_k=100, temperature=0.8\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Job"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 3.02s | 3 chars\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Give me a one-word synonym for 'Work'.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\": \"Top-K = 1 (Greedy ‚Äî always picks top token)\",  \"top_k\": 1,  \"temperature\": 0.8},\n",
    "    {\"label\": \"Top-K = 5\",                                     \"top_k\": 5,  \"temperature\": 0.8},\n",
    "    {\"label\": \"Top-K = 40 (Default for many models)\",          \"top_k\": 40, \"temperature\": 0.8},\n",
    "    {\"label\": \"Top-K = 100 (Wide pool)\",                       \"top_k\": 100, \"temperature\": 0.8},\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3B: Top-K Consistency Test\n",
    "\n",
    "With `top_k=1`, the output should be identical every time (greedy). Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  Top-K = 1 ‚Äî Running 4 times\n",
      "============================================================\n",
      "  Run 1: 4\n",
      "  Run 2: 4\n",
      "  Run 3: 4\n",
      "  Run 4: 4\n",
      "  ‚Üí Unique outputs: 1/4\n",
      "\n",
      "============================================================\n",
      "  Top-K = 50 ‚Äî Running 4 times\n",
      "============================================================\n",
      "  Run 1: 4\n",
      "  Run 2: 4\n",
      "  Run 3: 4\n",
      "  Run 4: 4\n",
      "  ‚Üí Unique outputs: 1/4\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is 2 + 2? Reply with just the number.\"\n",
    "num_runs = 4\n",
    "\n",
    "for k_val in [1, 50]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  Top-K = {k_val} ‚Äî Running {num_runs} times\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    outputs = []\n",
    "    for i in range(num_runs):\n",
    "        llm = ChatOllama(model=MODEL, top_k=k_val, temperature=0.8)\n",
    "        resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "        text = resp.content.strip()\n",
    "        outputs.append(text)\n",
    "        print(f\"  Run {i+1}: {text[:80]}\")\n",
    "    unique = len(set(outputs))\n",
    "    print(f\"  ‚Üí Unique outputs: {unique}/{num_runs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K vs. Top-P ‚Äî When to Use Which?\n",
    "\n",
    "| Feature | Top-K | Top-P |\n",
    "|---------|-------|-------|\n",
    "| Pool size | **Fixed** (always K tokens) | **Dynamic** (varies per step) |\n",
    "| Adapts to confidence? | No | Yes |\n",
    "| Best for | Simple control | Nuanced generation |\n",
    "| Common defaults | K=40 | P=0.9 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Max Tokens ‚Äî Controlling Response Length\n",
    "\n",
    "**Max Tokens** (called `num_predict` in Ollama) sets a hard ceiling on how many tokens the model generates. It does NOT guarantee that length ‚Äî the model may stop earlier if it finishes its thought.\n",
    "\n",
    "```\n",
    "1 token ‚âà 4 characters ‚âà ¬æ of a word (English)\n",
    "```\n",
    "\n",
    "### Experiment 4A: Varying Max Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  üîπ Max Tokens = 20 (Very Short)\n",
      "  Parameters: num_predict=20\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The theory of relativity is a physical theory that describes how space and time relate to matter, according"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 1.47s | 107 chars\n",
      "\n",
      "============================================================\n",
      "  üîπ Max Tokens = 80 (Short)\n",
      "  Parameters: num_predict=80\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The theory of relativity is a set of scientific theories developed by two physicists, Albert Einstein and Hendrik Lorentz, that describe how space and time relate to each other. The theory was first published in 1905 and has since become one of the most important theoretical frameworks in modern physics.\n",
       "\n",
       "One of the key ideas behind the theory is that space and time are not absolute quantities -"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 4.73s | 398 chars\n",
      "\n",
      "============================================================\n",
      "  üîπ Max Tokens = 300 (Medium)\n",
      "  Parameters: num_predict=300\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The theory of relativity is a set of scientific theories that describes how space and time affect objects moving at different speeds or in different gravitational fields, including acceleration. It was developed by two physicists, Albert Einstein, first as special relativity (1905) and then general relativity (1915).\n",
       "\n",
       "Special Relativity:\n",
       "The theory of special relativity introduced the concepts of spacetime and the principle of relativity to account for how the laws of physics apply in all inertial frames. It describes the effects that would occur if objects were observed moving at high speeds or near a strong gravitational field.\n",
       "\n",
       "Key ideas in special relativity include:\n",
       "\n",
       "1. The speed of light is constant, regardless of the motion of the observer.\n",
       "2. Time dilation: time slows down for an object moving at high speeds compared to observers on Earth.\n",
       "3. Length contraction: objects appear shorter than their length when they are moving fast relative to a stationary observer.\n",
       "4. Mass-energy equivalence: energy and mass can be converted into one another, as described by the equation E=mc^2.\n",
       "\n",
       "General Relativity:\n",
       "The theory of general relativity describes gravity as the curvature in spacetime caused by other massive objects occupying spacetime. This means that massive bodies warp spacetime, causing them to attract each other gravitationally because they are following geodesics through this curved space.\n",
       "\n",
       "Key ideas in general relativity include:\n",
       "\n",
       "1. Gravitational lensing: light bends around massive objects like stars or galaxies"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 23.97s | 1544 chars\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain the theory of relativity in detail.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\": \"üîπ Max Tokens = 20 (Very Short)\",   \"num_predict\": 20},\n",
    "    {\"label\": \"üîπ Max Tokens = 80 (Short)\",        \"num_predict\": 80},\n",
    "    {\"label\": \"üîπ Max Tokens = 300 (Medium)\",      \"num_predict\": 300},\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4B: Max Tokens ‚Äî Cutting Off Mid-Sentence\n",
    "\n",
    "Watch what happens when the limit is too low ‚Äî the model gets cut off mid-thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me a short story about a brave knight.\"\n",
    "\n",
    "for max_tok in [10, 30, 100]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  num_predict = {max_tok}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    _ = generate(prompt, num_predict=max_tok)\n",
    "\n",
    "print(\"\\nüí° Notice how low limits produce incomplete responses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Frequency Penalty ‚Äî Reducing Repetition\n",
    "\n",
    "Frequency Penalty penalizes tokens **proportionally to how many times** they've already appeared in the output. The more a word repeats, the harder it gets penalized.\n",
    "\n",
    "```\n",
    "Penalty = 0.0  ‚Üí  No penalty (default)\n",
    "Penalty > 0    ‚Üí  Discourages repetition (higher = stronger)\n",
    "Penalty < 0    ‚Üí  Encourages repetition (rarely useful)\n",
    "```\n",
    "\n",
    "In Ollama, this maps to the `repeat_penalty` parameter (default 1.1; values > 1.0 penalize repetition).\n",
    "\n",
    "### Experiment 5A: Repetition With and Without Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A prompt that tends to cause repetitive output\n",
    "prompt = \"Write the word 'hello' in 10 different creative ways.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\": \"üîÅ repeat_penalty = 1.0 (No penalty)\",       \"repeat_penalty\": 1.0},\n",
    "    {\"label\": \"‚öñÔ∏è repeat_penalty = 1.1 (Default / Mild)\",   \"repeat_penalty\": 1.1},\n",
    "    {\"label\": \"üö´ repeat_penalty = 1.5 (Strong penalty)\",   \"repeat_penalty\": 1.5},\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5B: Frequency Penalty on Longer Text\n",
    "\n",
    "Repetition is more visible in longer outputs. Let's test with a paragraph-length prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a paragraph about the importance of reading books. Aim for about 100 words.\"\n",
    "\n",
    "for penalty in [1.0, 1.3]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  repeat_penalty = {penalty}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    result = generate(prompt, repeat_penalty=penalty, num_predict=200)\n",
    "\n",
    "    # Count word frequency to show repetition\n",
    "    words = result.lower().split()\n",
    "    word_counts = {}\n",
    "    for w in words:\n",
    "        w_clean = w.strip('.,!?;:')\n",
    "        if len(w_clean) > 3:  # skip short words\n",
    "            word_counts[w_clean] = word_counts.get(w_clean, 0) + 1\n",
    "    repeated = {w: c for w, c in word_counts.items() if c >= 3}\n",
    "    if repeated:\n",
    "        print(f\"  üìä Words repeated 3+ times: {repeated}\")\n",
    "    else:\n",
    "        print(f\"  üìä No words repeated 3+ times ‚Äî good variety!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Presence Penalty ‚Äî Encouraging Topic Diversity\n",
    "\n",
    "Unlike Frequency Penalty (which scales with count), Presence Penalty applies a **flat penalty** to any token that has appeared **at least once**. It doesn't matter if it appeared 1 time or 50 ‚Äî the penalty is the same.\n",
    "\n",
    "```\n",
    "Frequency Penalty:  \"the\" appeared 5x ‚Üí penalized 5√ó as much\n",
    "Presence  Penalty:  \"the\" appeared 5x ‚Üí same penalty as if it appeared 1x\n",
    "```\n",
    "\n",
    "This encourages the model to bring in **new topics and vocabulary** rather than just avoiding repetition.\n",
    "\n",
    "> **Note:** In Ollama, `repeat_penalty` combines both frequency and presence penalty effects. We simulate the distinction below.\n",
    "\n",
    "### Experiment 6A: Presence Penalty Effect on Vocabulary Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  repeat_penalty = 1.0 (No penalty)\n",
      "  Parameters: repeat_penalty=1.0, temperature=0.7\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1. Elephant\n",
       "2. Giraffe\n",
       "3. Monkey\n",
       "4. Lion\n",
       "5. Tiger\n",
       "6. Penguin\n",
       "7. Dolphin\n",
       "8. Fox\n",
       "9. Kangaroo\n",
       "10. Octopus"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 2.38s | 102 chars\n",
      "\n",
      "============================================================\n",
      "  repeat_penalty = 1.2 (Moderate)\n",
      "  Parameters: repeat_penalty=1.2, temperature=0.7\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Elephant  \n",
       "Lion  \n",
       "Tiger  \n",
       "Bear  \n",
       "Monkey  \n",
       "Penguin  \n",
       "Kangaroo  \n",
       "Giraffe  \n",
       "Eagle  \n",
       "Crocodile"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è±Ô∏è 2.05s | 90 chars\n",
      "\n",
      "============================================================\n",
      "  repeat_penalty = 1.8 (Aggressive)\n",
      "  Parameters: repeat_penalty=1.8, temperature=0.7\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"List 10 different animals. Just the names, one per line.\"\n",
    "\n",
    "configs = [\n",
    "    {\"label\": \"repeat_penalty = 1.0 (No penalty)\",    \"repeat_penalty\": 1.0, \"temperature\": 0.7},\n",
    "    {\"label\": \"repeat_penalty = 1.2 (Moderate)\",      \"repeat_penalty\": 1.2, \"temperature\": 0.7},\n",
    "    {\"label\": \"repeat_penalty = 1.8 (Aggressive)\",    \"repeat_penalty\": 1.8, \"temperature\": 0.7},\n",
    "]\n",
    "\n",
    "results = compare(prompt, configs)\n",
    "\n",
    "# Analyze unique words in each\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"VOCABULARY DIVERSITY ANALYSIS\")\n",
    "print(f\"{'=' * 60}\")\n",
    "for label, text in results.items():\n",
    "    words = set(text.lower().split())\n",
    "    print(f\"  {label[:40]:40s} ‚Üí {len(words)} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency vs. Presence Penalty ‚Äî Comparison\n",
    "\n",
    "| Aspect | Frequency Penalty | Presence Penalty |\n",
    "|--------|-------------------|------------------|\n",
    "| Scales with count? | **Yes** ‚Äî more repetitions = more penalty | **No** ‚Äî flat penalty after first use |\n",
    "| Best for | Reducing word-level repetition | Encouraging topic diversity |\n",
    "| Use case | Preventing \"the the the...\" | Making model explore new ideas |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Stop Sequences ‚Äî Halting Generation\n",
    "\n",
    "Stop sequences are strings that **immediately end** the model's generation when encountered. The model stops *before* including the stop string in the output.\n",
    "\n",
    "Common uses:\n",
    "- Stop at a newline (`\\n`) for single-line answers\n",
    "- Stop at a delimiter (`---`, `END`) for structured extraction\n",
    "- Stop at a role marker (`User:`) to prevent the model from simulating conversation\n",
    "\n",
    "### Experiment 7A: Stopping at a Newline (Single-Line Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Name a famous scientist and describe their contribution.\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WITHOUT stop sequence\")\n",
    "print(\"=\" * 60)\n",
    "_ = generate(prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WITH stop=['.'] ‚Äî stops at first period\")\n",
    "print(\"=\" * 60)\n",
    "_ = generate(prompt, stop=[\".\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7B: Stop Sequences for Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Extract the person's name from the text below.\n",
    "\n",
    "Text: \"Dr. Sarah Chen published her findings on climate change last Tuesday.\"\n",
    "\n",
    "Name:\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WITH stop=['\\\\n'] ‚Äî stops after extracting the name\")\n",
    "print(\"=\" * 60)\n",
    "_ = generate(prompt, stop=[\"\\n\"], temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7C: Stop Sequences to Prevent Role-Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the user's question in one sentence.\n",
    "\n",
    "User: What is gravity?\n",
    "Assistant:\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WITHOUT stop ‚Äî model might continue as 'User:' and 'Assistant:'\")\n",
    "print(\"=\" * 60)\n",
    "_ = generate(prompt, num_predict=200)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WITH stop=['User:', '\\\\n\\\\n'] ‚Äî halts after one response\")\n",
    "print(\"=\" * 60)\n",
    "_ = generate(prompt, stop=[\"User:\", \"\\n\\n\"], num_predict=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Combining Parameters ‚Äî Real-World Recipes\n",
    "\n",
    "In practice, you'll combine multiple parameters together. Here are some common \"recipes\":\n",
    "\n",
    "| Use Case | Temperature | Top-P | Top-K | Repeat Penalty | Max Tokens |\n",
    "|----------|-------------|-------|-------|----------------|------------|\n",
    "| Factual Q&A | 0.0 | 1.0 | 1 | 1.0 | 100-200 |\n",
    "| Creative Writing | 0.9-1.2 | 0.9 | 50 | 1.2 | 500+ |\n",
    "| Code Generation | 0.0-0.2 | 0.95 | 40 | 1.1 | 500 |\n",
    "| Brainstorming | 1.0 | 0.95 | 100 | 1.3 | 300 |\n",
    "| Data Extraction | 0.0 | 1.0 | 1 | 1.0 | 100 |\n",
    "\n",
    "### Experiment 8A: Recipe Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Suggest 3 startup ideas related to artificial intelligence.\"\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        \"label\": \"üìã Conservative (Factual style)\",\n",
    "        \"temperature\": 0.1, \"top_p\": 1.0, \"top_k\": 1, \"repeat_penalty\": 1.0, \"num_predict\": 200\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"‚öñÔ∏è Balanced (General purpose)\",\n",
    "        \"temperature\": 0.7, \"top_p\": 0.9, \"top_k\": 40, \"repeat_penalty\": 1.1, \"num_predict\": 200\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"üöÄ Creative (Brainstorming)\",\n",
    "        \"temperature\": 1.1, \"top_p\": 0.95, \"top_k\": 100, \"repeat_penalty\": 1.3, \"num_predict\": 200\n",
    "    }\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8B: Code Generation Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a Python function that checks if a string is a palindrome.\"\n",
    "\n",
    "system = \"You are a Python developer. Write clean, well-commented code. Only output the code, nothing else.\"\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        \"label\": \"üéØ Precise Code (temp=0, top_k=1)\",\n",
    "        \"temperature\": 0.0, \"top_k\": 1, \"num_predict\": 300\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"üé® Creative Code (temp=0.8, top_k=50)\",\n",
    "        \"temperature\": 0.8, \"top_k\": 50, \"num_predict\": 300\n",
    "    },\n",
    "]\n",
    "\n",
    "_ = compare(prompt, configs, system=system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Sandbox ‚Äî Try It Yourself!\n",
    "\n",
    "Experiment with any combination of parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  SANDBOX - Tweak these values and re-run!\n",
    "# ============================================================\n",
    "\n",
    "my_prompt     = \"Describe the future of space travel in 3 sentences.\"\n",
    "my_system     = \"You are a futurist and science communicator.\"\n",
    "\n",
    "my_params = {\n",
    "    \"temperature\":    0.7,    # 0.0 to 2.0\n",
    "    \"top_p\":          0.9,    # 0.0 to 1.0\n",
    "    \"top_k\":          40,     # 1 to 100+\n",
    "    \"num_predict\":    200,    # max tokens to generate\n",
    "    \"repeat_penalty\": 1.1,    # 1.0 = off, higher = less repetition\n",
    "    # \"stop\":         [\".\"],  # uncomment to stop at first period\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "print(\"YOUR CUSTOM EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "params_str = '\\n'.join(f\"  {k:20s} = {v}\" for k, v in my_params.items())\n",
    "print(params_str)\n",
    "print(\"=\" * 60)\n",
    "_ = generate(my_prompt, system=my_system, **my_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Parameter | What It Does | Typical Range | When to Adjust |\n",
    "|-----------|-------------|---------------|----------------|\n",
    "| **Temperature** | Controls randomness | 0.0 ‚Äì 1.5 | Lower for facts, higher for creativity |\n",
    "| **Top-P** | Dynamic probability cutoff | 0.1 ‚Äì 1.0 | Use ~0.9 for general; lower for precision |\n",
    "| **Top-K** | Fixed candidate pool size | 1 ‚Äì 100 | 1 for greedy; 40-50 for balanced |\n",
    "| **Max Tokens** | Hard output length limit | 10 ‚Äì 4096 | Match to your expected output length |\n",
    "| **Frequency Penalty** | Penalizes repeated tokens proportionally | 1.0 ‚Äì 1.5 | Increase for less repetition |\n",
    "| **Presence Penalty** | Flat penalty on any used token | 1.0 ‚Äì 1.5 | Increase for broader vocabulary |\n",
    "| **Stop Sequences** | Halts generation at specific strings | N/A | Use for structured/single-line output |\n",
    "\n",
    "### Rules of Thumb\n",
    "\n",
    "1. **Start with defaults** ‚Äî temperature=0.7, top_p=0.9, top_k=40\n",
    "2. **Adjust one parameter at a time** ‚Äî so you can see what each one does\n",
    "3. **Temperature and Top-P overlap** ‚Äî usually tune one or the other, not both aggressively\n",
    "4. **Low temperature + Top-K=1** ‚Äî effectively deterministic (greedy decoding)\n",
    "5. **Stop sequences are underused** ‚Äî they're great for structured extraction tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
