{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 OpenAI API Deep Dive — Vision Capabilities (GPT-4o-mini)\n",
    "\n",
    "## Playground Notebook\n",
    "\n",
    "GPT-4o-mini is a **multimodal** model — it can see and understand images alongside text.\n",
    "\n",
    "| Capability | Examples |\n",
    "|------------|----------|\n",
    "| **Describe images** | What's in this photo? |\n",
    "| **Read text in images** | OCR from screenshots, documents, signs |\n",
    "| **Analyze charts** | Interpret data visualizations |\n",
    "| **Compare images** | Spot differences between two images |\n",
    "| **Reason about visuals** | What's wrong with this UI? |\n",
    "\n",
    "### Two Ways to Send Images\n",
    "\n",
    "| Method | When to Use |\n",
    "|--------|-------------|\n",
    "| **URL** | Image is publicly hosted online |\n",
    "| **Base64** | Local files not hosted anywhere |\n",
    "\n",
    "> **Model:** `gpt-4o-mini` — supports vision at lower cost.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client ready | Model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown, HTML, Image\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "client = OpenAI()\n",
    "\n",
    "print(f\"\\u2705 Client ready | Model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vision helpers loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def vision_url(prompt, image_url, detail=\"low\", max_tokens=100):\n",
    "    \"\"\"Send an image URL to the model for analysis.\"\"\"\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url, \"detail\": detail}}\n",
    "            ]\n",
    "        }],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    content = response.choices[0].message.content\n",
    "    display(Markdown(content))\n",
    "    print(f\"\\n\\u23f1\\ufe0f {elapsed:.2f}s | Tokens: {response.usage.total_tokens}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def vision_base64(prompt, image_path, detail=\"low\", max_tokens=100):\n",
    "    \"\"\"Send a local image file as base64.\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    # Detect mime type from extension\n",
    "    ext = os.path.splitext(image_path)[1].lower()\n",
    "    mime = {\"jpg\": \"jpeg\", \"jpeg\": \"jpeg\", \"png\": \"png\", \"gif\": \"gif\", \"webp\": \"webp\"}\n",
    "    media_type = mime.get(ext.lstrip(\".\"), \"png\")\n",
    "\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\n",
    "                    \"url\": f\"data:image/{media_type};base64,{b64}\",\n",
    "                    \"detail\": detail\n",
    "                }}\n",
    "            ]\n",
    "        }],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    content = response.choices[0].message.content\n",
    "    display(Markdown(content))\n",
    "    print(f\"\\n\\u23f1\\ufe0f {elapsed:.2f}s | Tokens: {response.usage.total_tokens}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "print(\"\\u2705 Vision helpers loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Image URL — Analyzing Online Images\n",
    "\n",
    "### Experiment 1A: Describe an Image from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/refs/heads/master/ComputerVision/Images/dog.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asking GPT-4o-mini to describe this image...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The image features a small brown dachshund standing on a wooden deck at sunset. The soft glow of the sunset creates a warm ambiance, enhancing the dog's features and showcasing its alert expression."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 2.05s | Tokens: 2886\n"
     ]
    }
   ],
   "source": [
    "# Image URL\n",
    "IMAGE_URL = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/refs/heads/master/ComputerVision/Images/dog.jpg\"\n",
    "\n",
    "# Show the image in notebook\n",
    "display(Image(url=IMAGE_URL, width=400))\n",
    "\n",
    "print(\"\\nAsking GPT-4o-mini to describe this image...\\n\")\n",
    "_ = vision_url(\"Describe this image in 2 sentences.\", IMAGE_URL, max_tokens=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1B: Reading Text from an Image (OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asking model to read the text...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello from Python! API Key: sk-abc123"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 1.08s | Tokens: 2859\n"
     ]
    }
   ],
   "source": [
    "# Create a simple image with text using PIL\n",
    "try:\n",
    "    from PIL import Image as PILImage, ImageDraw, ImageFont\n",
    "\n",
    "    img = PILImage.new('RGB', (400, 100), color='white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((20, 30), \"Hello from Python! API Key: sk-abc123\", fill='black')\n",
    "    img.save('test_ocr.png')\n",
    "\n",
    "    display(Image(filename='test_ocr.png'))\n",
    "    print(\"\\nAsking model to read the text...\\n\")\n",
    "    _ = vision_base64(\"Read all text in this image exactly.\", \"test_ocr.png\", max_tokens=40)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"PIL not installed. Using a URL image for OCR demo instead.\")\n",
    "    # Fallback: use any public image with text\n",
    "    _ = vision_url(\n",
    "        \"Read any text visible in this image.\",\n",
    "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2f/Google_2015_logo.svg/800px-Google_2015_logo.svg.png\",\n",
    "        max_tokens=30\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Base64 — Sending Local Image Files\n",
    "\n",
    "### Experiment 2A: Analyze a Local Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ image.png not found. Place an image file here to test local analysis.\n",
      "   The base64 encoding pattern is shown in the helper function above.\n"
     ]
    }
   ],
   "source": [
    "# Check if image.png exists in the project directory\n",
    "local_image = \"image.png\"\n",
    "\n",
    "if os.path.exists(local_image):\n",
    "    display(Image(filename=local_image, width=400))\n",
    "    print(f\"\\nAnalyzing {local_image}...\\n\")\n",
    "    _ = vision_base64(\"Describe what you see in this image. Be brief.\", local_image, max_tokens=80)\n",
    "else:\n",
    "    print(f\"\\u26a0\\ufe0f {local_image} not found. Place an image file here to test local analysis.\")\n",
    "    print(\"   The base64 encoding pattern is shown in the helper function above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2B: How Base64 Encoding Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size:     544 bytes\n",
      "Base64 length: 728 characters\n",
      "Base64 ratio:  1.3x (base64 is ~33% larger)\n",
      "\n",
      "First 80 chars: iVBORw0KGgoAAAANSUhEUgAAAGQAAAAyCAIAAAAlV+npAAAB50lEQVR4nO3ZO0hbURzH8f893rTmQUzU...\n",
      "\n",
      "The URL sent to the API:\n",
      "  data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAyCAIAAAAl...\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the encoding process\n",
    "try:\n",
    "    from PIL import Image as PILImage, ImageDraw\n",
    "\n",
    "    # Create a tiny test image\n",
    "    img = PILImage.new('RGB', (100, 50), color='#3498db')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((10, 15), \"Test\", fill='white')\n",
    "    img.save('tiny_test.png')\n",
    "\n",
    "    # Show the encoding\n",
    "    with open('tiny_test.png', 'rb') as f:\n",
    "        raw_bytes = f.read()\n",
    "        b64_string = base64.b64encode(raw_bytes).decode('utf-8')\n",
    "\n",
    "    print(f\"File size:     {len(raw_bytes)} bytes\")\n",
    "    print(f\"Base64 length: {len(b64_string)} characters\")\n",
    "    print(f\"Base64 ratio:  {len(b64_string)/len(raw_bytes):.1f}x (base64 is ~33% larger)\")\n",
    "    print(f\"\\nFirst 80 chars: {b64_string[:80]}...\")\n",
    "    print(f\"\\nThe URL sent to the API:\")\n",
    "    print(f\"  data:image/png;base64,{b64_string[:40]}...\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"PIL not available. The concept: read file bytes \\u2192 base64 encode \\u2192 embed in URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Detail Levels — Cost vs. Accuracy\n",
    "\n",
    "The `detail` parameter controls how the model processes the image:\n",
    "\n",
    "| Level | Resolution | Tokens | Best For |\n",
    "|-------|-----------|--------|----------|\n",
    "| `low` | Fixed 512x512 | ~85 tokens | Quick descriptions, thumbnails |\n",
    "| `high` | Up to 2048x2048 | 85-1105 tokens | Reading small text, detailed analysis |\n",
    "| `auto` | Model decides | Varies | General use |\n",
    "\n",
    "### Experiment 3A: Low vs High Detail Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "  detail='low'\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The image features three colorful dice in blue, green, and red, each adorned with white dots."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 1.40s | Tokens: 2867\n",
      "\n",
      "==================================================\n",
      "  detail='high'\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The image features three colorful dice—red, blue, and green—displaying white dots against a vibrant, multicolored background."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 1.54s | Tokens: 25542\n",
      "\n",
      "ℹ️ Notice the token count difference — 'high' costs more but sees more detail.\n"
     ]
    }
   ],
   "source": [
    "TEST_URL = \"https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png\"\n",
    "\n",
    "for detail_level in [\"low\", \"high\"]:\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"  detail='{detail_level}'\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    _ = vision_url(\n",
    "        \"Describe this in 1 sentence.\",\n",
    "        TEST_URL,\n",
    "        detail=detail_level,\n",
    "        max_tokens=50\n",
    "    )\n",
    "\n",
    "print(\"\\n\\u2139\\ufe0f Notice the token count difference — 'high' costs more but sees more detail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Practical Use Cases\n",
    "\n",
    "### Experiment 4A: Structured Data Extraction from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Vision + JSON Mode — Structured extraction from an image\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"kind\": \"Dachshund\",\n",
       "  \"rarity\": \"Common\",\n",
       "  \"notable features\": [\"Long body\", \"Short legs\", \"Distinctive ear shape\"]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 1.76s | Tokens: 2905\n",
      "⚠️ Response was not strict JSON — but the data is still extracted above.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Combine vision with structured output\n",
    "print(\"=\" * 60)\n",
    "print(\"Vision + JSON Mode — Structured extraction from an image\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use a reliably accessible image URL; some Wikimedia thumbnails return 403 errors\n",
    "img_url = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/refs/heads/master/ComputerVision/Images/dog.jpg\"\n",
    "\n",
    "response = vision_url(\n",
    "    \"Identify this type of dog . Return JSON with keys: kind, rarity, notable features. JSON only.\",\n",
    "    img_url,\n",
    "    max_tokens=80\n",
    ")\n",
    "\n",
    "text = response.choices[0].message.content\n",
    "try:\n",
    "    parsed = json.loads(text)\n",
    "    print(f\"\\n\\u2705 Valid JSON! Parsed:\")\n",
    "    print(json.dumps(parsed, indent=2))\n",
    "except json.JSONDecodeError:\n",
    "    print(\"\\u26a0\\ufe0f Response was not strict JSON — but the data is still extracted above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4B: Multiple Images in One Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Multiple Images — Comparing two logos\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The first image features a dachshund standing on a wooden deck during sunset, showcasing a peaceful outdoor setting. In contrast, the second image displays colorful dice floating against a vibrant background, conveying a sense of playfulness and chance associated with games."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 2.03s | Tokens: 5682+49=5731\n"
     ]
    }
   ],
   "source": [
    "# Send two images for comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"Multiple Images — Comparing two logos\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# We need a custom call here since vision_url handles single images\n",
    "# The original Wikimedia SVG thumbnails sometimes return 403; use guaranteed-access URLs\n",
    "img1 = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/refs/heads/master/ComputerVision/Images/dog.jpg\"\n",
    "img2 = \"https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png\"\n",
    "\n",
    "start = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Compare these two images in 2 sentences.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": img1, \"detail\": \"low\"}},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": img2, \"detail\": \"low\"}}\n",
    "        ]\n",
    "    }],\n",
    "    max_tokens=80\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "print(f\"\\n\\u23f1\\ufe0f {elapsed:.2f}s | Tokens: {response.usage.prompt_tokens}+{response.usage.completion_tokens}={response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4C: Multi-Turn Vision — Model Remembers the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TURN 1 — Send image + question\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This is a dachshund, a breed of dog known for its long body and short legs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 1.01s | Tokens: 2868\n",
      "\n",
      "============================================================\n",
      "TURN 2 — Follow-up (no image resent)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The dachshund in the image is a reddish-brown color."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ 1.12s | Tokens: 2881+14=2895\n",
      "\n",
      "✅ The model remembers the image from Turn 1 — no need to resend it!\n"
     ]
    }
   ],
   "source": [
    "# Use a URL that the API can fetch; some Wikimedia URLs return 403 errors.\n",
    "IMG = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-sample-data-files/refs/heads/master/ComputerVision/Images/dog.jpg\"\n",
    "\n",
    "# Turn 1: Send the image\n",
    "print(\"=\" * 60)\n",
    "print(\"TURN 1 — Send image + question\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What animal is this? One sentence.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": IMG, \"detail\": \"low\"}}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "r1 = vision_url(\"What animal is this? One sentence.\", IMG, max_tokens=40)\n",
    "a1 = r1.choices[0].message.content\n",
    "\n",
    "# Turn 2: Follow-up WITHOUT resending the image\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"TURN 2 — Follow-up (no image resent)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": a1})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What color is it?\"})\n",
    "\n",
    "start = time.time()\n",
    "r2 = client.chat.completions.create(model=MODEL, messages=messages, max_tokens=40)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "display(Markdown(r2.choices[0].message.content))\n",
    "print(f\"\\n\\u23f1\\ufe0f {elapsed:.2f}s | Tokens: {r2.usage.prompt_tokens}+{r2.usage.completion_tokens}={r2.usage.total_tokens}\")\n",
    "print(f\"\\n\\u2705 The model remembers the image from Turn 1 — no need to resend it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Limitations\n",
    "\n",
    "| Limitation | Details |\n",
    "|------------|--------|\n",
    "| **No spatial precision** | Cannot give exact pixel coordinates |\n",
    "| **Small text** | May struggle with tiny text — use `detail=\"high\"` |\n",
    "| **Specialized images** | Medical/satellite imagery — not trained for professional diagnosis |\n",
    "| **CAPTCHA / faces** | Will refuse for safety reasons |\n",
    "| **Image size** | Max 20MB per image; auto-resized to fit token budget |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | What to Remember |\n",
    "|---------|------------------|\n",
    "| **Multimodal** | GPT-4o-mini handles images + text in the same request |\n",
    "| **URL method** | `{\"type\": \"image_url\", \"image_url\": {\"url\": \"...\"}}` |\n",
    "| **Base64 method** | `data:image/png;base64,{encoded}` for local files |\n",
    "| **detail** | `low` = cheap/fast (85 tokens), `high` = detailed (up to 1105 tokens) |\n",
    "| **Multiple images** | Send several images in one `content` array |\n",
    "| **Multi-turn** | Image is remembered — no need to resend in follow-up turns |\n",
    "| **+ Structured** | Combine vision with JSON mode for data extraction |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
