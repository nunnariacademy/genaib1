{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 OpenAI API Deep Dive — Rate Limiting, Error Handling & Best Practices\n",
    "\n",
    "## Playground Notebook\n",
    "\n",
    "Production applications **will** encounter errors. This notebook teaches you to handle every one of them.\n",
    "\n",
    "| Topic | What You'll Learn |\n",
    "|-------|-------------------|\n",
    "| **Rate Limits** | What they are, how tiers work, reading headers |\n",
    "| **Error Types** | Every error you'll encounter + whether to retry |\n",
    "| **Retry Logic** | Exponential backoff with jitter |\n",
    "| **Best Practices** | Timeouts, token management, cost optimization |\n",
    "\n",
    "> **Model:** `gpt-4o-mini`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Client ready | Model: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "client = OpenAI()\n",
    "\n",
    "print(f\"\\u2705 Client ready | Model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helpers loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def chat(messages, max_tokens=150, **kwargs):\n",
    "    \"\"\"Send messages to OpenAI and display the response.\"\"\"\n",
    "    start = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        **kwargs\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    content = response.choices[0].message.content\n",
    "    if content:\n",
    "        display(Markdown(content))\n",
    "    print(f\"\\n\\u23f1\\ufe0f {elapsed:.2f}s | Tokens: {response.usage.prompt_tokens}+{response.usage.completion_tokens}={response.usage.total_tokens}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "def show_messages(messages):\n",
    "    \"\"\"Pretty-print the message list being sent.\"\"\"\n",
    "    colors = {\"system\": \"#e74c3c\", \"user\": \"#3498db\", \"assistant\": \"#2ecc71\", \"tool\": \"#f39c12\"}\n",
    "    html = \"\"\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, dict):\n",
    "            role = msg.get(\"role\", \"unknown\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "        else:\n",
    "            role = getattr(msg, \"role\", \"unknown\")\n",
    "            content = getattr(msg, \"content\", \"(empty)\")\n",
    "        if content and len(str(content)) > 200:\n",
    "            content = str(content)[:200] + \"...\"\n",
    "        color = colors.get(role, \"#888\")\n",
    "        html += (\n",
    "            f'<div style=\"margin:6px 0;padding:8px 12px;border-left:4px solid {color};'\n",
    "            f'background:#1e1e1e;border-radius:4px;\">'\n",
    "            f'<strong style=\"color:{color};text-transform:uppercase;\">{role}</strong>'\n",
    "            f'<br><span style=\"color:#ccc;\">{content}</span></div>'\n",
    "        )\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "print(\"\\u2705 Helpers loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rate Limits — Understanding the Boundaries\n",
    "\n",
    "OpenAI enforces limits across **three dimensions**:\n",
    "\n",
    "| Dimension | What It Measures |\n",
    "|-----------|------------------|\n",
    "| **RPM** | Requests Per Minute |\n",
    "| **TPM** | Tokens Per Minute |\n",
    "| **RPD** | Requests Per Day |\n",
    "\n",
    "### Rate Limit Tiers\n",
    "\n",
    "| Tier | RPM | TPM | Triggered By |\n",
    "|------|-----|-----|-------------|\n",
    "| Free | 3 | 40,000 | New account |\n",
    "| Tier 1 | 500 | 200,000 | $5 paid |\n",
    "| Tier 2 | 5,000 | 2,000,000 | $50 paid |\n",
    "| Tier 3 | 5,000 | 4,000,000 | $100 paid |\n",
    "\n",
    "### Experiment 1A: Reading Rate Limit Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─ RATE LIMIT HEADERS ────────────────────────────┐\n",
      "│ Max requests/min          5000\n",
      "│ Max tokens/min            2000000\n",
      "│ Remaining requests        4999\n",
      "│ Remaining tokens          1999997\n",
      "│ Requests reset in         12ms\n",
      "│ Tokens reset in           0s\n",
      "└────────────────────────────────────────────────┘\n",
      "\n",
      "Response: Hello! How can I\n"
     ]
    }
   ],
   "source": [
    "# Make a request and inspect the raw HTTP response headers\n",
    "response = client.chat.completions.with_raw_response.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "    max_tokens=5\n",
    ")\n",
    "\n",
    "# Extract rate limit headers\n",
    "headers = response.headers\n",
    "print(\"\\u250c\\u2500 RATE LIMIT HEADERS \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\")\n",
    "\n",
    "rate_headers = [\n",
    "    (\"x-ratelimit-limit-requests\",     \"Max requests/min\"),\n",
    "    (\"x-ratelimit-limit-tokens\",       \"Max tokens/min\"),\n",
    "    (\"x-ratelimit-remaining-requests\", \"Remaining requests\"),\n",
    "    (\"x-ratelimit-remaining-tokens\",   \"Remaining tokens\"),\n",
    "    (\"x-ratelimit-reset-requests\",     \"Requests reset in\"),\n",
    "    (\"x-ratelimit-reset-tokens\",       \"Tokens reset in\"),\n",
    "]\n",
    "\n",
    "for header, label in rate_headers:\n",
    "    value = headers.get(header, \"N/A\")\n",
    "    print(f\"\\u2502 {label:25s} {value}\")\n",
    "\n",
    "print(f\"\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\")\n",
    "\n",
    "# Parse the actual completion too\n",
    "completion = response.parse()\n",
    "print(f\"\\nResponse: {completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Error Types — Every Error You Will Encounter\n",
    "\n",
    "| Error | Status | Retry? | Action |\n",
    "|-------|--------|--------|--------|\n",
    "| `AuthenticationError` | 401 | \\u274c Never | Fix your API key |\n",
    "| `PermissionDeniedError` | 403 | \\u274c Never | Check permissions |\n",
    "| `NotFoundError` | 404 | \\u274c Never | Fix model name/endpoint |\n",
    "| `BadRequestError` | 400 | \\u274c Never | Fix your request |\n",
    "| `RateLimitError` | 429 | \\u2705 Yes | Wait + retry with backoff |\n",
    "| `InternalServerError` | 500 | \\u2705 Yes | Retry with backoff |\n",
    "| `APIConnectionError` | - | \\u2705 Yes | Check network |\n",
    "| `APITimeoutError` | - | \\u2705 Yes | Increase timeout |\n",
    "\n",
    "### Experiment 2A: Triggering and Catching Common Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. AuthenticationError (bad key)\n",
      "   ❌ Caught! Status 401: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-invalid.\n",
      "\n",
      "2. NotFoundError (wrong model name)\n",
      "   ❌ Caught! Status 404: model not found\n",
      "\n",
      "3. BadRequestError (invalid request)\n",
      "   ❌ Caught! Status 400: Error code: 400 - {'error': {'message': \"Invalid 'messages': empty array. Expect\n",
      "\n",
      "✅ All error types caught correctly!\n"
     ]
    }
   ],
   "source": [
    "from openai import (\n",
    "    AuthenticationError,\n",
    "    BadRequestError,\n",
    "    NotFoundError,\n",
    "    RateLimitError,\n",
    "    APIConnectionError,\n",
    "    APITimeoutError,\n",
    "    APIError\n",
    ")\n",
    "\n",
    "# --- 1. AuthenticationError (401) ---\n",
    "print(\"1. AuthenticationError (bad key)\")\n",
    "try:\n",
    "    bad = OpenAI(api_key=\"sk-invalid\")\n",
    "    bad.chat.completions.create(\n",
    "        model=MODEL, messages=[{\"role\": \"user\", \"content\": \"Hi\"}], max_tokens=5\n",
    "    )\n",
    "except AuthenticationError as e:\n",
    "    print(f\"   \\u274c Caught! Status {e.status_code}: {str(e)[:80]}\")\n",
    "\n",
    "# --- 2. NotFoundError (404) ---\n",
    "print(\"\\n2. NotFoundError (wrong model name)\")\n",
    "try:\n",
    "    client.chat.completions.create(\n",
    "        model=\"gpt-99-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hi\"}], max_tokens=5\n",
    "    )\n",
    "except NotFoundError as e:\n",
    "    print(f\"   \\u274c Caught! Status {e.status_code}: model not found\")\n",
    "\n",
    "# --- 3. BadRequestError (400) ---\n",
    "print(\"\\n3. BadRequestError (invalid request)\")\n",
    "try:\n",
    "    client.chat.completions.create(\n",
    "        model=MODEL, messages=[], max_tokens=5  # empty messages!\n",
    "    )\n",
    "except BadRequestError as e:\n",
    "    print(f\"   \\u274c Caught! Status {e.status_code}: {str(e)[:80]}\")\n",
    "\n",
    "print(\"\\n\\u2705 All error types caught correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2B: The Complete Error Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"margin:6px 0;padding:8px 12px;border-left:4px solid #3498db;background:#1e1e1e;border-radius:4px;\"><strong style=\"color:#3498db;text-transform:uppercase;\">user</strong><br><span style=\"color:#ccc;\">Say hello in 3 words.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello there, friend!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ Tokens: 14+5=19\n",
      "✅ safe_chat worked!\n"
     ]
    }
   ],
   "source": [
    "def safe_chat(messages, max_tokens=80):\n",
    "    \"\"\"Make an API call with comprehensive error handling.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL, messages=messages, max_tokens=max_tokens\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        display(Markdown(content))\n",
    "        print(f\"\\n\\u23f1\\ufe0f Tokens: {response.usage.prompt_tokens}+{response.usage.completion_tokens}={response.usage.total_tokens}\")\n",
    "        return content\n",
    "\n",
    "    except AuthenticationError:\n",
    "        print(\"\\u274c Auth failed — check your API key\")\n",
    "    except BadRequestError as e:\n",
    "        print(f\"\\u274c Bad request — fix your input: {e}\")\n",
    "    except NotFoundError:\n",
    "        print(f\"\\u274c Model '{MODEL}' not found\")\n",
    "    except RateLimitError:\n",
    "        print(\"\\u26a0\\ufe0f Rate limited — should retry with backoff\")\n",
    "    except APIConnectionError:\n",
    "        print(\"\\u26a0\\ufe0f Connection error — check internet\")\n",
    "    except APITimeoutError:\n",
    "        print(\"\\u26a0\\ufe0f Timeout — try increasing timeout or simplifying request\")\n",
    "    except APIError as e:\n",
    "        print(f\"\\u26a0\\ufe0f Server error ({e.status_code}) — retry\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Test with a valid request\n",
    "messages = [{\"role\": \"user\", \"content\": \"Say hello in 3 words.\"}]\n",
    "show_messages(messages)\n",
    "result = safe_chat(messages)\n",
    "print(f\"\\u2705 safe_chat worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Retry Logic — Exponential Backoff with Jitter\n",
    "\n",
    "When a **retriable** error occurs, wait before retrying — each attempt waits longer.\n",
    "\n",
    "```\n",
    "Attempt 1: wait 1s\n",
    "Attempt 2: wait 2s\n",
    "Attempt 3: wait 4s  (doubles each time)\n",
    "```\n",
    "\n",
    "**Jitter** adds randomness to prevent all clients retrying at the same moment (thundering herd problem).\n",
    "\n",
    "### Experiment 3A: Manual Exponential Backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"margin:6px 0;padding:8px 12px;border-left:4px solid #3498db;background:#1e1e1e;border-radius:4px;\"><strong style=\"color:#3498db;text-transform:uppercase;\">user</strong><br><span style=\"color:#ccc;\">Name a planet. One word.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Mars."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏱️ Tokens: 14+2=16\n",
      "✅ Succeeded on first try!\n"
     ]
    }
   ],
   "source": [
    "def chat_with_retry(messages, max_retries=3, max_tokens=80):\n",
    "    \"\"\"Chat with exponential backoff + jitter for retriable errors.\"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL, messages=messages, max_tokens=max_tokens\n",
    "            )\n",
    "            content = response.choices[0].message.content\n",
    "            display(Markdown(content))\n",
    "            print(f\"\\n\\u23f1\\ufe0f Tokens: {response.usage.prompt_tokens}+{response.usage.completion_tokens}={response.usage.total_tokens}\")\n",
    "            return content\n",
    "\n",
    "        except (RateLimitError, APIConnectionError, APITimeoutError) as e:\n",
    "            if attempt == max_retries:\n",
    "                print(f\"\\u274c All {max_retries} retries exhausted.\")\n",
    "                raise\n",
    "\n",
    "            # Exponential backoff with jitter\n",
    "            base_wait = 2 ** attempt  # 1, 2, 4, 8...\n",
    "            jitter = random.uniform(0, base_wait * 0.5)\n",
    "            wait = base_wait + jitter\n",
    "\n",
    "            print(f\"  \\u26a0\\ufe0f Attempt {attempt + 1} failed: {type(e).__name__}\")\n",
    "            print(f\"     Waiting {wait:.1f}s before retry...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        except (AuthenticationError, BadRequestError, NotFoundError) as e:\n",
    "            # Non-retriable — fail immediately\n",
    "            print(f\"\\u274c Non-retriable error: {type(e).__name__}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Test with a valid request (should succeed on first try)\n",
    "messages = [{\"role\": \"user\", \"content\": \"Name a planet. One word.\"}]\n",
    "show_messages(messages)\n",
    "result = chat_with_retry(messages)\n",
    "print(f\"\\u2705 Succeeded on first try!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3B: Using the `tenacity` Library (Production Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ tenacity result: Crimson.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "    @retry(\n",
    "        wait=wait_exponential(multiplier=1, min=1, max=10),\n",
    "        stop=stop_after_attempt(3),\n",
    "        retry=retry_if_exception_type((RateLimitError, APIConnectionError, APITimeoutError)),\n",
    "        before_sleep=lambda retry_state: print(f\"  \\u26a0\\ufe0f Retrying in {retry_state.next_action.sleep:.1f}s...\")\n",
    "    )\n",
    "    def chat_tenacity(messages, max_tokens=80):\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL, messages=messages, max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    result = chat_tenacity([{\"role\": \"user\", \"content\": \"Name a color. One word.\"}])\n",
    "    print(f\"\\u2705 tenacity result: {result}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\u26a0\\ufe0f tenacity not installed. Install with: pip install tenacity\")\n",
    "    print(\"   The manual retry pattern above works just as well.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Backoff Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential Backoff Timing (simulated):\n",
      "Attempt    Base Wait    With Jitter    Cumulative\n",
      "--------------------------------------------------\n",
      "1          1.0          1.16           1.2s  ██\n",
      "2          2.0          2.77           3.9s  █████\n",
      "3          4.0          4.40           8.3s  ████████\n",
      "4          8.0          10.37          18.7s  ████████████████████\n",
      "5          16.0         16.83          35.5s  █████████████████████████████████\n",
      "6          32.0         36.97          72.5s  █████████████████████████████████████████████████████████████████████████\n",
      "\n",
      "ℹ️ After 6 retries, total wait ≈ 72s\n"
     ]
    }
   ],
   "source": [
    "# Visualize what exponential backoff with jitter looks like\n",
    "print(\"Exponential Backoff Timing (simulated):\")\n",
    "print(f\"{'Attempt':<10} {'Base Wait':<12} {'With Jitter':<14} {'Cumulative'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cumulative = 0\n",
    "for i in range(6):\n",
    "    base = 2 ** i\n",
    "    jitter = random.uniform(0, base * 0.5)\n",
    "    actual = base + jitter\n",
    "    cumulative += actual\n",
    "    bar = \"\\u2588\" * int(actual * 2)\n",
    "    print(f\"{i+1:<10} {base:<12.1f} {actual:<14.2f} {cumulative:.1f}s  {bar}\")\n",
    "\n",
    "print(f\"\\n\\u2139\\ufe0f After 6 retries, total wait \\u2248 {cumulative:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Best Practices\n",
    "\n",
    "### Experiment 4A: Setting Timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Response: Hi! How can I assist you today?\n",
      "\n",
      "Recommended Timeouts:\n",
      "  Use Case                       Timeout\n",
      "  ------------------------------ ----------\n",
      "  Short chat completion          10-15s\n",
      "  Long generation                30-60s\n",
      "  Function calling               30s\n",
      "  Whisper transcription          60s\n",
      "  TTS generation                 30s\n"
     ]
    }
   ],
   "source": [
    "# Client with custom timeout\n",
    "timed_client = OpenAI(timeout=30.0)  # 30 second timeout\n",
    "\n",
    "# Or per-request timeout\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say hi.\"}],\n",
    "    max_tokens=10,\n",
    "    timeout=10.0  # 10 second timeout for this request only\n",
    ")\n",
    "print(f\"\\u2705 Response: {response.choices[0].message.content}\")\n",
    "\n",
    "print(\"\\nRecommended Timeouts:\")\n",
    "print(f\"  {'Use Case':<30} {'Timeout'}\")\n",
    "print(f\"  {'-'*30} {'-'*10}\")\n",
    "print(f\"  {'Short chat completion':<30} {'10-15s'}\")\n",
    "print(f\"  {'Long generation':<30} {'30-60s'}\")\n",
    "print(f\"  {'Function calling':<30} {'30s'}\")\n",
    "print(f\"  {'Whisper transcription':<30} {'60s'}\")\n",
    "print(f\"  {'TTS generation':<30} {'30s'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4B: Counting Tokens Before Sending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text                                                                   Tokens\n",
      "--------------------------------------------------------------------------------\n",
      "Hello                                                                  1\n",
      "Explain quantum computing                                              3\n",
      "Write a detailed essay about the history of artificial intelligen...   17\n",
      "\n",
      "ℹ️ Tokenization of: \"OpenAI's GPT-4o-mini is cost-effective.\"\n",
      "   Tokens (12): ['Open', 'AI', \"'s\", ' GPT', '-', '4', 'o', '-mini', ' is', ' cost', '-effective', '.']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "\n",
    "    enc = tiktoken.encoding_for_model(MODEL)\n",
    "\n",
    "    texts = [\n",
    "        \"Hello\",\n",
    "        \"Explain quantum computing\",\n",
    "        \"Write a detailed essay about the history of artificial intelligence from its origins to modern day.\",\n",
    "    ]\n",
    "\n",
    "    print(f\"{'Text':<70} {'Tokens'}\")\n",
    "    print(\"-\" * 80)\n",
    "    for text in texts:\n",
    "        tokens = enc.encode(text)\n",
    "        display_text = text[:65] + \"...\" if len(text) > 65 else text\n",
    "        print(f\"{display_text:<70} {len(tokens)}\")\n",
    "\n",
    "    # Show tokenization\n",
    "    sample = \"OpenAI's GPT-4o-mini is cost-effective.\"\n",
    "    tokens = enc.encode(sample)\n",
    "    print(f\"\\n\\u2139\\ufe0f Tokenization of: \\\"{sample}\\\"\")\n",
    "    print(f\"   Tokens ({len(tokens)}): {[enc.decode([t]) for t in tokens]}\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\u26a0\\ufe0f tiktoken not installed. Install with: pip install tiktoken\")\n",
    "    print(\"   Rule of thumb: 1 token \\u2248 4 characters \\u2248 0.75 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4C: Tracking API Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \"What is AI?\" → $0.000020\n",
      "  \"Name 3 colors.\" → $0.000006\n",
      "  \"Say hello.\" → $0.000007\n",
      "\n",
      "┌─ COST SUMMARY ───────────────────┐\n",
      "│ Requests:      3\n",
      "│ Input tokens:  33\n",
      "│ Output tokens: 46\n",
      "│ Total cost:    $0.000033\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Simple cost tracker\n",
    "class CostTracker:\n",
    "    # gpt-4o-mini pricing (per 1M tokens)\n",
    "    PRICING = {\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-4o\":      {\"input\": 2.50, \"output\": 10.00},\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.total_input = 0\n",
    "        self.total_output = 0\n",
    "        self.total_cost = 0.0\n",
    "        self.requests = 0\n",
    "\n",
    "    def track(self, response):\n",
    "        usage = response.usage\n",
    "        model = response.model\n",
    "        prices = self.PRICING.get(model, self.PRICING[\"gpt-4o-mini\"])\n",
    "\n",
    "        input_cost = (usage.prompt_tokens / 1_000_000) * prices[\"input\"]\n",
    "        output_cost = (usage.completion_tokens / 1_000_000) * prices[\"output\"]\n",
    "        cost = input_cost + output_cost\n",
    "\n",
    "        self.total_input += usage.prompt_tokens\n",
    "        self.total_output += usage.completion_tokens\n",
    "        self.total_cost += cost\n",
    "        self.requests += 1\n",
    "        return cost\n",
    "\n",
    "    def summary(self):\n",
    "        print(f\"\\u250c\\u2500 COST SUMMARY \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510\")\n",
    "        print(f\"\\u2502 Requests:      {self.requests}\")\n",
    "        print(f\"\\u2502 Input tokens:  {self.total_input:,}\")\n",
    "        print(f\"\\u2502 Output tokens: {self.total_output:,}\")\n",
    "        print(f\"\\u2502 Total cost:    ${self.total_cost:.6f}\")\n",
    "        print(f\"\\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518\")\n",
    "\n",
    "\n",
    "tracker = CostTracker()\n",
    "\n",
    "# Make a few requests and track costs\n",
    "for q in [\"What is AI?\", \"Name 3 colors.\", \"Say hello.\"]:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": q}],\n",
    "        max_tokens=30\n",
    "    )\n",
    "    cost = tracker.track(resp)\n",
    "    print(f\"  \\\"{q}\\\" \\u2192 ${cost:.6f}\")\n",
    "\n",
    "print()\n",
    "tracker.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4D: Managing Conversation History Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before trim: 17 messages\n",
      "  ✂️ Trimmed 17 → 7 messages\n",
      "After trim:  7 messages\n",
      "Kept: system + messages ['Message 6', 'Reply 6', 'Message 7', 'Reply 7', 'Message 8', 'Reply 8']\n"
     ]
    }
   ],
   "source": [
    "def trim_history(messages, max_messages=10):\n",
    "    \"\"\"Keep conversation within bounds. Always preserve system message.\"\"\"\n",
    "    if len(messages) <= max_messages:\n",
    "        return messages\n",
    "\n",
    "    # Keep system message + last N messages\n",
    "    system = [m for m in messages if m[\"role\"] == \"system\"]\n",
    "    non_system = [m for m in messages if m[\"role\"] != \"system\"]\n",
    "    trimmed = system + non_system[-(max_messages - len(system)):]\n",
    "\n",
    "    print(f\"  \\u2702\\ufe0f Trimmed {len(messages)} \\u2192 {len(trimmed)} messages\")\n",
    "    return trimmed\n",
    "\n",
    "\n",
    "# Simulate a growing conversation\n",
    "history = [{\"role\": \"system\", \"content\": \"Be concise.\"}]\n",
    "\n",
    "for i in range(8):\n",
    "    history.append({\"role\": \"user\", \"content\": f\"Message {i+1}\"})\n",
    "    history.append({\"role\": \"assistant\", \"content\": f\"Reply {i+1}\"})\n",
    "\n",
    "print(f\"Before trim: {len(history)} messages\")\n",
    "trimmed = trim_history(history, max_messages=7)\n",
    "print(f\"After trim:  {len(trimmed)} messages\")\n",
    "print(f\"Kept: system + messages {[m['content'] for m in trimmed[1:]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Production-Ready Client Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 4\n",
      "Result: Mango.\n",
      "\n",
      "┌─ COST SUMMARY ───────────────────┐\n",
      "│ Requests:      2\n",
      "│ Input tokens:  32\n",
      "│ Output tokens: 4\n",
      "│ Total cost:    $0.000007\n",
      "└─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "class RobustOpenAI:\n",
    "    \"\"\"Production-ready OpenAI client with retry, timeout, and cost tracking.\"\"\"\n",
    "\n",
    "    def __init__(self, model=\"gpt-4o-mini\", max_retries=3, timeout=30.0):\n",
    "        self.client = OpenAI(timeout=timeout)\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        self.tracker = CostTracker()\n",
    "\n",
    "    def chat(self, messages, max_tokens=100):\n",
    "        for attempt in range(self.max_retries + 1):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model, messages=messages, max_tokens=max_tokens\n",
    "                )\n",
    "                self.tracker.track(response)\n",
    "                return response.choices[0].message.content\n",
    "\n",
    "            except (RateLimitError, APIConnectionError, APITimeoutError) as e:\n",
    "                if attempt == self.max_retries:\n",
    "                    raise\n",
    "                wait = (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"  Retry {attempt+1}/{self.max_retries} in {wait:.1f}s ({type(e).__name__})\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "            except (AuthenticationError, BadRequestError, NotFoundError):\n",
    "                raise  # don't retry\n",
    "\n",
    "\n",
    "# Use it\n",
    "ai = RobustOpenAI()\n",
    "\n",
    "result = ai.chat([{\"role\": \"user\", \"content\": \"What is 2+2? Just the number.\"}], max_tokens=10)\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "result = ai.chat([{\"role\": \"user\", \"content\": \"Name a fruit. One word.\"}], max_tokens=10)\n",
    "print(f\"Result: {result}\")\n",
    "\n",
    "print()\n",
    "ai.tracker.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "| Concept | What to Remember |\n",
    "|---------|------------------|\n",
    "| **Rate Limits** | RPM, TPM, RPD — check headers to monitor |\n",
    "| **Tiers** | Limits grow automatically as you spend more |\n",
    "| **Non-retriable** | 401, 403, 404, 400 — fix the code, don't retry |\n",
    "| **Retriable** | 429, 500, connection, timeout — retry with backoff |\n",
    "| **Backoff** | Exponential: 1s, 2s, 4s, 8s... + random jitter |\n",
    "| **Jitter** | Random offset prevents thundering herd |\n",
    "| **Timeouts** | Set per-client or per-request (10-60s typical) |\n",
    "| **Token counting** | Use `tiktoken` before sending to estimate cost |\n",
    "| **History trim** | Keep conversation size bounded to manage costs |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
